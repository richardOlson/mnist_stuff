{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "tensorflow_env",
   "display_name": "tensorflow_env",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import os\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "# importing the operator module to do some comparisons\n",
    "import operator\n",
    "import datetime\n",
    "import random\n",
    "from chunking import create_weight_avg, reshape_data, chunk_shuffle, chunk_size, load_images, get_loss_or_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing the loading of the tensorboard\n",
    "#%load_ext tensorboard\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(flatten=False):\n",
    "    \"\"\"\n",
    "    The function to create a model\n",
    "    \"\"\"\n",
    "    inputs = None\n",
    "    if flatten:\n",
    "        inputs = keras.layers.Flatten(shape=(28,28))\n",
    "    else:\n",
    "        inputs = keras.layers.Input(shape=(784,))\n",
    "    t = keras.layers.Dense(512, activation=\"relu\", )(inputs)\n",
    "    t = keras.layers.Dropout(.2)(t)\n",
    "    out = keras.layers.Dense(10, )(t)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=out)\n",
    "\n",
    "   \n",
    "    # doing the compilation of the model\n",
    "    model.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the model\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'.\\\\..\\\\checkPoints/cp.ckpt'"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "# getting the path\n",
    "path = os.path.join(os.path.curdir, \"..\", \"checkPoints/cp.ckpt\")\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"functional_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         [(None, 784)]             0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 512)               401920    \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 512)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 10)                5130      \n=================================================================\nTotal params: 407,050\nTrainable params: 407,050\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# showing the summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The shape of what is returned with the weigths is ---- 0\nThe type is --- <class 'list'>\nThese are the weights for the input_1 layer --- []\nThe shape of what is returned with the weigths is ---- 2\nThe type is --- <class 'list'>\nThe type for each item in the list is: <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\nThese are the weights for the dense layer --- [<tf.Variable 'dense/kernel:0' shape=(784, 512) dtype=float32, numpy=\narray([[-0.05626954, -0.05369684, -0.06167276, ..., -0.001957  ,\n         0.00331007,  0.05483547],\n       [-0.049044  , -0.04101725,  0.04828054, ...,  0.0366065 ,\n         0.00066533,  0.05718373],\n       [ 0.00024945,  0.05019872, -0.0286467 , ...,  0.05689124,\n        -0.01853205,  0.05268286],\n       ...,\n       [-0.02224023, -0.06074744,  0.06550466, ..., -0.06264395,\n        -0.02794588, -0.01869341],\n       [ 0.03836417, -0.06636625, -0.0272741 , ...,  0.06481206,\n         0.06491137,  0.03532643],\n       [-0.02355461, -0.01153382,  0.01450439, ..., -0.02485488,\n        -0.00436073,  0.04444816]], dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(512,) dtype=float32, numpy=\narray([ 1.72597487e-04,  1.05121862e-02, -9.93606634e-03,  1.65048260e-02,\n        1.48517517e-02,  1.02724703e-02,  2.61683874e-02,  7.80888041e-03,\n       -6.67838706e-03, -2.63294880e-03,  7.54132587e-03,  1.07546821e-02,\n       -3.55655327e-04,  4.24108701e-03,  6.80069160e-03,  9.35562234e-03,\n       -1.48517676e-02, -1.26193035e-02,  2.14740597e-02,  1.90498773e-02,\n        1.75802875e-02,  1.00164497e-02,  2.33117156e-02,  5.82871586e-03,\n        1.24599226e-02,  2.49987701e-03, -4.81321290e-03, -3.32437549e-03,\n        2.52118185e-02,  1.74722690e-02,  5.97302278e-04,  5.02793957e-03,\n        1.51756210e-02,  8.94126110e-03,  1.02106249e-02,  1.77973378e-02,\n        1.91553961e-02, -4.73733991e-03,  6.50379900e-03, -1.48366541e-02,\n        9.10403393e-03,  5.40261390e-03,  1.62270386e-02,  1.17752440e-02,\n       -5.23779681e-03, -6.07746793e-03,  5.50066493e-03,  3.80394142e-03,\n        1.03695886e-02,  1.43970409e-02,  9.94092785e-03,  9.76029492e-04,\n       -7.81322550e-03, -2.01762130e-04,  3.46075930e-03,  1.20185986e-02,\n       -1.87506294e-03,  6.11349475e-03,  1.68360770e-02,  1.68225840e-02,\n       -2.46017892e-03, -1.42869249e-03,  1.85877215e-02,  1.65404603e-02,\n        2.38595940e-02,  1.01817586e-02,  2.15113480e-02,  1.31973531e-02,\n       -5.73000079e-03,  1.58557054e-02, -4.49281745e-03,  2.04001972e-03,\n        4.20698710e-03,  1.88715048e-02,  4.06323420e-03, -3.21356405e-04,\n        1.24803148e-02,  1.13657508e-02, -1.42590879e-02, -6.26258180e-03,\n        8.94737802e-03,  1.17486324e-02, -1.33981407e-02,  6.00477168e-03,\n        3.67667386e-03,  1.21088652e-02,  1.48060801e-03,  1.78699940e-02,\n        1.67217135e-04,  4.69582807e-03,  1.36005282e-02,  2.41139308e-02,\n       -1.46972360e-02,  1.36002703e-02,  8.92073568e-03,  2.23400593e-02,\n        5.55964792e-03,  1.95225626e-02,  1.62121840e-02,  3.65433225e-04,\n        2.03223117e-02,  9.67860874e-03, -6.90692523e-03,  1.18965907e-02,\n        1.19682113e-02, -4.15906496e-03,  8.81340820e-03, -1.83345713e-02,\n        1.10727884e-02,  1.21528478e-02, -1.50002241e-02,  1.79905035e-02,\n        1.85826402e-02, -3.98032740e-03, -4.49450081e-03,  1.26722511e-02,\n        1.90368434e-03,  3.97697091e-03, -1.27569510e-04,  7.51278829e-03,\n       -4.52209311e-03,  8.58710706e-03,  1.38698379e-02, -2.46764021e-03,\n        1.09259719e-02, -5.99331968e-03, -6.49669115e-03,  2.59255469e-02,\n        1.07646296e-02, -9.94778238e-04,  1.35442580e-03,  6.43221429e-03,\n        1.55213065e-02, -1.03907716e-02,  1.67679079e-02,  1.48895886e-02,\n       -1.18476804e-02,  1.07257171e-02,  3.55914212e-03,  1.11360867e-02,\n        2.93036830e-03, -3.82516207e-03, -7.80127384e-03, -2.39326619e-03,\n        2.46371757e-02, -1.27290459e-02,  4.07224707e-03,  2.12547034e-02,\n        1.63449980e-02, -1.96036268e-02,  1.01368725e-02,  5.91298752e-03,\n       -5.23868762e-03,  1.61788203e-02, -4.67349915e-03,  1.06823221e-02,\n        5.90169919e-04, -6.95585785e-03,  1.34356255e-02,  9.71553940e-03,\n        1.97174270e-02,  1.70956999e-02,  1.06709301e-02, -6.54631713e-03,\n       -7.34526431e-04,  1.74862556e-02,  1.42350337e-02, -6.98204851e-04,\n        1.40222372e-03,  9.99701302e-03, -5.94796031e-04, -3.38633684e-03,\n        6.83624577e-03,  2.06210911e-02,  6.89667277e-03, -9.13625769e-03,\n        2.23413692e-03,  1.46558462e-02, -1.52699575e-02,  1.26199508e-02,\n       -3.74557939e-03,  1.70344543e-02,  1.93049088e-02, -9.92825907e-03,\n        1.24239745e-02,  1.85128339e-02, -1.59667118e-03,  1.87675506e-02,\n        1.22579010e-02,  2.29209736e-02,  1.78051554e-02,  4.82282881e-03,\n        1.65875424e-02,  1.39386905e-02,  1.37819564e-02, -6.30658527e-04,\n       -9.30631813e-03,  1.08812954e-02,  1.87493283e-02,  1.71661433e-02,\n        1.36542949e-03,  3.62640317e-03,  8.66733212e-03,  2.22192612e-02,\n       -9.54966247e-03,  2.52975114e-02, -6.86078239e-03,  3.23635247e-03,\n        2.44395831e-03,  1.17151979e-02,  5.28266234e-03,  4.39155614e-03,\n        6.83129486e-03,  2.26570442e-02,  1.96919180e-02, -5.14318841e-03,\n       -1.46707119e-02,  1.50537351e-02, -7.96610583e-03, -5.84941404e-03,\n       -1.04225511e-02, -3.48156667e-03,  6.20870618e-03,  8.47083516e-04,\n       -3.22766340e-04,  2.02458296e-02,  8.79207999e-03, -1.08482623e-02,\n       -4.10871720e-03,  1.82971377e-02,  1.92662328e-02,  2.38690572e-03,\n       -3.01049603e-03, -7.71063007e-03,  7.57430261e-03,  5.64594427e-03,\n        4.10549762e-03,  1.92663120e-03, -3.68727138e-03, -1.50720710e-02,\n        4.26016469e-03,  8.37763958e-03,  1.87635794e-02,  1.72189120e-02,\n        1.94656663e-02,  7.15053687e-03,  8.85615405e-03, -1.42346381e-03,\n       -9.99983400e-03,  2.16865949e-02, -1.71931449e-03,  3.17133451e-03,\n        3.99380783e-03,  1.99397691e-02,  2.40188669e-02,  1.31260483e-02,\n        3.68883112e-03,  1.27733145e-02, -3.24683590e-03, -3.65647790e-03,\n        2.10799966e-02,  1.31309489e-02,  1.81905795e-02,  5.53172082e-04,\n        1.09740598e-02,  1.67485494e-02,  1.38644679e-02,  2.56677456e-02,\n       -5.92459692e-03, -9.01204161e-03,  1.50966719e-02,  1.90052949e-02,\n        5.04563795e-03,  1.95423011e-02, -2.22481205e-03,  2.00304314e-02,\n       -2.42300448e-03,  1.94869041e-02,  1.60062574e-02,  1.85983982e-02,\n        1.06929727e-02, -7.58236181e-03,  2.66808104e-02, -1.34435343e-02,\n        1.13502573e-02,  2.58019683e-03, -9.92083363e-03, -1.13562159e-02,\n        2.50601186e-03,  1.24915186e-02,  1.46082081e-02,  9.80041642e-03,\n        2.44212858e-02,  4.38334094e-03,  2.25721393e-03,  2.59431899e-02,\n        1.03705528e-03,  5.60802687e-03,  2.73208413e-02,  1.78735913e-03,\n       -7.89626210e-05,  5.90488361e-03,  9.59941931e-03,  7.09745847e-03,\n        5.42883063e-03,  1.11090923e-02, -1.65096018e-03,  9.30715818e-03,\n        1.25081968e-02,  5.28563047e-03,  1.38063738e-02,  1.55692799e-02,\n        1.94110665e-02,  1.20160030e-02,  1.19931754e-02, -9.86145250e-03,\n       -4.20161895e-03,  1.11806933e-02, -1.16814449e-02,  1.95057283e-03,\n       -1.19410164e-03, -5.52692974e-04,  1.86904371e-02,  7.18024885e-03,\n        2.42474792e-03,  5.96098136e-04,  2.52153887e-03,  2.20747739e-02,\n        2.95085032e-02,  1.31511874e-02,  2.25789733e-02, -1.11722788e-04,\n        1.16808983e-02,  1.92412268e-02,  9.21589043e-03,  3.73866945e-03,\n        8.38876050e-03,  2.29975265e-02,  1.25467060e-02, -8.58375989e-03,\n        1.19069275e-02,  1.16868911e-03, -3.79161863e-03,  2.32451450e-04,\n        1.03091856e-03,  1.29203321e-02, -4.12527053e-03,  8.03037488e-04,\n        6.71883160e-03,  3.42380581e-03,  7.29215425e-03, -5.46654360e-03,\n        5.38758375e-03, -4.01928835e-03,  1.97195783e-02, -1.42078269e-02,\n        1.46365268e-02,  1.39304148e-02, -7.20077660e-05, -3.99321085e-03,\n        1.80188045e-02,  1.18029630e-02, -5.73757617e-03, -1.67114806e-04,\n        2.41491292e-03, -6.95342082e-04,  2.10326128e-02,  7.40283588e-03,\n       -1.23865912e-02,  8.13089591e-03,  3.47751984e-03,  1.52790127e-02,\n        1.62633304e-02,  6.99118851e-03,  1.67725086e-02,  2.04010010e-02,\n        7.17896549e-03, -6.63141068e-03, -2.67516077e-03, -3.48725333e-03,\n       -1.08487699e-02, -1.05346544e-02, -1.14916568e-03, -4.90661850e-03,\n        2.67180521e-03,  2.25566644e-02,  7.95148686e-03,  8.24715383e-03,\n       -6.75756112e-03, -6.13528304e-03,  1.26109244e-02,  1.65690240e-02,\n       -9.37484019e-03,  1.85696762e-02,  3.04171462e-02,  8.15941207e-03,\n        1.68669783e-02,  1.80480856e-04,  1.92198344e-02, -1.38751874e-02,\n        2.82430113e-03,  7.58931227e-03, -5.92449505e-05,  1.37020685e-02,\n        1.05779839e-03,  1.03384741e-02,  1.86582841e-02,  1.13291712e-02,\n       -2.57410631e-02,  6.47610752e-03,  2.68482175e-02,  2.21051555e-03,\n        3.20901396e-03,  7.53921317e-03, -7.31907692e-03,  1.43468492e-02,\n        2.40712725e-02,  5.22164768e-03,  8.10789689e-03,  1.42931417e-02,\n       -1.13822753e-02,  4.85085184e-03,  3.26441508e-03,  5.15708118e-04,\n        1.23437187e-02,  2.07592417e-02,  2.08405964e-03,  2.08609216e-02,\n       -7.50211300e-03,  2.43883003e-02,  1.71769746e-02,  3.45202279e-04,\n        7.74060842e-03,  1.75596327e-02,  1.49875330e-02,  1.00264782e-02,\n        2.27080137e-02, -4.31349734e-03, -8.22057715e-04, -5.30624529e-03,\n        1.44437999e-02,  1.01032453e-02,  3.02405711e-02,  1.30343810e-02,\n       -9.96138249e-03, -1.17465463e-02,  6.86999783e-03,  1.69459078e-02,\n        8.81494116e-03,  1.89694893e-02, -8.15362297e-03,  2.04268526e-02,\n        2.05217451e-02,  2.41035428e-02,  1.63802225e-02,  2.44297888e-02,\n        6.81259716e-03, -4.27511724e-04, -8.88006855e-03,  1.69149449e-03,\n       -1.00014545e-02, -2.10044812e-03,  2.84275389e-03,  5.16199833e-03,\n       -2.96031102e-03, -9.93147027e-03,  9.39159933e-03, -7.03254715e-04,\n       -5.56486985e-03,  1.94572415e-02,  5.54063357e-04,  1.27571467e-02,\n        6.32402534e-03,  1.54216932e-02,  2.88840686e-03,  4.95937327e-03,\n        1.87297799e-02, -1.37654459e-02,  3.01342495e-02,  1.98066961e-02,\n        4.67774924e-03,  1.97523683e-02,  3.84773174e-03,  1.51203284e-02,\n        1.42727140e-02,  2.30135377e-02, -1.43146319e-02,  1.01353768e-02,\n       -7.84549024e-03,  1.54151777e-02, -2.93102057e-04,  3.77435004e-03,\n        2.01666355e-02,  8.91412981e-03,  1.88908970e-03,  1.84706245e-02,\n       -1.27008101e-02,  1.77656468e-02,  4.96036559e-03,  6.27404312e-03,\n        2.41043326e-02, -9.00016259e-03,  2.04670690e-02,  4.75749187e-03,\n        1.76593731e-03,  1.06164468e-02,  1.41866617e-02, -2.15561278e-02,\n        4.11901297e-03, -1.51740536e-02,  9.53926146e-03,  2.51788497e-02],\n      dtype=float32)>]\nThe shape of what is returned with the weigths is ---- 0\nThe type is --- <class 'list'>\nThese are the weights for the dropout layer --- []\nThe shape of what is returned with the weigths is ---- 2\nThe type is --- <class 'list'>\nThe type for each item in the list is: <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\nThese are the weights for the dense_1 layer --- [<tf.Variable 'dense_1/kernel:0' shape=(512, 10) dtype=float32, numpy=\narray([[ 0.03622122, -0.03641613,  0.12541647, ..., -0.03731978,\n        -0.03972755, -0.00030406],\n       [-0.09065956, -0.04421225, -0.00835247, ..., -0.06340674,\n        -0.14152284, -0.06728777],\n       [ 0.02464028, -0.04258672,  0.04578171, ..., -0.04067839,\n         0.03545604, -0.0039114 ],\n       ...,\n       [ 0.04221679,  0.08953761,  0.11619351, ...,  0.02745421,\n        -0.02400982, -0.07182321],\n       [-0.09616829, -0.06718431,  0.09197892, ..., -0.07621801,\n        -0.06656138, -0.13127476],\n       [ 0.00644363,  0.08017394,  0.00601754, ...,  0.12485438,\n        -0.04525638, -0.0403614 ]], dtype=float32)>, <tf.Variable 'dense_1/bias:0' shape=(10,) dtype=float32, numpy=\narray([-0.02103808,  0.00912677, -0.00462178, -0.01466964,  0.01683323,\n        0.01430766, -0.00607732,  0.01283323, -0.01082663, -0.00280234],\n      dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "# printing the weights of the layers\n",
    "counter = 0\n",
    "for layer in model.layers:\n",
    "    print(f\"The shape of what is returned with the weigths is ---- {len(layer.weights)}\")\n",
    "    print(f\"The type is --- {type(layer.weights)}\")\n",
    "    if len(layer.weights) > 0:\n",
    "        print(f\"The type for each item in the list is: {type(layer.weights[0])}\")\n",
    "    print(f\"These are the weights for the {layer.name} layer --- {layer.weights}\" )\n",
    "    counter += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((1000, 784), (1000,))"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# now creating a model\n",
    "# looking at the shape of the train \n",
    "train_images.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 1.5915 - sparse_categorical_accuracy: 0.5840 - val_loss: 0.9791 - val_sparse_categorical_accuracy: 0.8110\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.5774 - sparse_categorical_accuracy: 0.8940 - val_loss: 0.6365 - val_sparse_categorical_accuracy: 0.8230\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.3004 - sparse_categorical_accuracy: 0.9370 - val_loss: 0.5017 - val_sparse_categorical_accuracy: 0.8500\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.2014 - sparse_categorical_accuracy: 0.9550 - val_loss: 0.4593 - val_sparse_categorical_accuracy: 0.8700\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1432 - sparse_categorical_accuracy: 0.9710 - val_loss: 0.4497 - val_sparse_categorical_accuracy: 0.8640\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1147 - sparse_categorical_accuracy: 0.9780 - val_loss: 0.4310 - val_sparse_categorical_accuracy: 0.8740\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0952 - sparse_categorical_accuracy: 0.9840 - val_loss: 0.4405 - val_sparse_categorical_accuracy: 0.8710\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0772 - sparse_categorical_accuracy: 0.9910 - val_loss: 0.4264 - val_sparse_categorical_accuracy: 0.8850\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0632 - sparse_categorical_accuracy: 0.9920 - val_loss: 0.4189 - val_sparse_categorical_accuracy: 0.8840\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0535 - sparse_categorical_accuracy: 0.9940 - val_loss: 0.4269 - val_sparse_categorical_accuracy: 0.8770\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x255cdd512e0>"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "model2.fit(x=train_images, y=train_labels, batch_size=128, epochs=10, \n",
    "            validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(list, tensorflow.python.ops.resource_variable_ops.ResourceVariable)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# getting the weights for this model\n",
    "the_weights = model2.weights\n",
    "type(the_weights), type(the_weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Variable 'dense/kernel:0' shape=(784, 512) dtype=float32, numpy=\n",
       "array([[ 2.0674616e-02,  3.2345042e-02, -7.7739358e-05, ...,\n",
       "        -6.2627956e-02, -3.8801931e-02, -1.3471089e-02],\n",
       "       [ 2.7597107e-02,  5.6585222e-02,  1.0034814e-03, ...,\n",
       "         3.5107583e-03, -3.3186216e-02, -1.6239803e-02],\n",
       "       [ 2.3986742e-02,  2.8475918e-02,  1.3272785e-02, ...,\n",
       "        -1.3933085e-02,  1.9999221e-03,  1.2072645e-02],\n",
       "       ...,\n",
       "       [ 8.9056566e-03,  3.4107566e-02, -5.5254426e-02, ...,\n",
       "        -5.1518083e-03,  1.5794642e-02,  3.3432893e-02],\n",
       "       [-2.1421652e-02,  2.4194144e-02,  2.5040790e-02, ...,\n",
       "        -2.1030013e-02,  6.0876384e-02, -6.0057208e-02],\n",
       "       [-3.4105852e-03,  2.0341501e-03,  6.6730469e-02, ...,\n",
       "        -4.5968406e-02,  2.5523305e-02,  5.9543997e-02]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "the_weights[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(numpy.ndarray, (784, 512))"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# getting the weights of the model\n",
    "w = the_weights[0].numpy()\n",
    "type(w), w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(numpy.ndarray, (784, 512))"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "# getting the weights of the first model above\n",
    "first_w = model.weights[0].numpy()\n",
    "type(first_w), w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "# getting the average of the two weights\n",
    "avg_wt = (w + first_w)/2\n",
    "type(avg_wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'list'> <class 'numpy.ndarray'> 4\n-first is (784, 512), second (512,), third (512, 10), 4th is (10,)\n"
     ]
    }
   ],
   "source": [
    "# getting the item of the tensorflow variables\n",
    "the_var_weights = model2.get_weights()\n",
    "print(type(the_var_weights),  type(the_var_weights[0]), len(the_var_weights))\n",
    "print(f\"-first is {the_var_weights[0].shape}, second {the_var_weights[1].shape}, third {the_var_weights[2].shape}, 4th is {the_var_weights[3].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the data and then will reshape the data\n",
    "train, test = load_images()\n",
    "# reshaping the data\n",
    "end = len(train)\n",
    "train_images1, train_labels1 = reshape_data(train,start_index=-1000, end_index=len(train[0]))\n",
    "test_images1, test_labels1 = reshape_data(test, start_index=-1000, end_index=len(test[0]))\n",
    "\n",
    "train_images2 , train_labels2 = reshape_data(train, start_index=0, end_index=1000)\n",
    "test_images2, test_labels2 = reshape_data(test, start_index=0, end_index=1000)\n",
    "\n",
    "train_images3 , train_labels3 = reshape_data(train, start_index=3000, end_index=4000)\n",
    "test_images3, test_labels3 = reshape_data(test, start_index=3000, end_index=4000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((1000, 784), (1000, 784), (1000, 784), (1000, 784), (1000, 784))"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "# checking the shapes of all the items\n",
    "train_images1.shape, train_images2.shape, train_images3.shape, test_images1.shape, test_images3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((1000, 784), (1000, 784), (1000, 784))"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# looking at the test data\n",
    "test_images1.shape, test_images2.shape, test_images3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now building all the models\n",
    "model1 = create_model()\n",
    "model2 = create_model()\n",
    "model3 = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.6440 - sparse_categorical_accuracy: 0.5480 - val_loss: 1.0485 - val_sparse_categorical_accuracy: 0.7940\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.6156 - sparse_categorical_accuracy: 0.8970 - val_loss: 0.6332 - val_sparse_categorical_accuracy: 0.8350\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.3124 - sparse_categorical_accuracy: 0.9350 - val_loss: 0.5292 - val_sparse_categorical_accuracy: 0.8440\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.2010 - sparse_categorical_accuracy: 0.9530 - val_loss: 0.4623 - val_sparse_categorical_accuracy: 0.8580\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1521 - sparse_categorical_accuracy: 0.9660 - val_loss: 0.4428 - val_sparse_categorical_accuracy: 0.8730\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1193 - sparse_categorical_accuracy: 0.9750 - val_loss: 0.4277 - val_sparse_categorical_accuracy: 0.8680\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0923 - sparse_categorical_accuracy: 0.9850 - val_loss: 0.4365 - val_sparse_categorical_accuracy: 0.8670\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0780 - sparse_categorical_accuracy: 0.9880 - val_loss: 0.4192 - val_sparse_categorical_accuracy: 0.8730\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0646 - sparse_categorical_accuracy: 0.9930 - val_loss: 0.4144 - val_sparse_categorical_accuracy: 0.8760\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0585 - sparse_categorical_accuracy: 0.9930 - val_loss: 0.4299 - val_sparse_categorical_accuracy: 0.8750\n"
     ]
    }
   ],
   "source": [
    "# now doing the trainig of each of the models\n",
    "h1 = model1.fit(x=train_images1, y=train_labels1, batch_size=128, epochs=10,\n",
    "            validation_data=(test_images1, test_labels1))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 1.7181 - sparse_categorical_accuracy: 0.5150 - val_loss: 1.1943 - val_sparse_categorical_accuracy: 0.7420\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.7917 - sparse_categorical_accuracy: 0.8290 - val_loss: 0.7779 - val_sparse_categorical_accuracy: 0.7720\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.4924 - sparse_categorical_accuracy: 0.8780 - val_loss: 0.6217 - val_sparse_categorical_accuracy: 0.8060\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.3733 - sparse_categorical_accuracy: 0.9040 - val_loss: 0.5303 - val_sparse_categorical_accuracy: 0.8280\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.2884 - sparse_categorical_accuracy: 0.9270 - val_loss: 0.4817 - val_sparse_categorical_accuracy: 0.8450\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.2365 - sparse_categorical_accuracy: 0.9410 - val_loss: 0.4629 - val_sparse_categorical_accuracy: 0.8490\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.2072 - sparse_categorical_accuracy: 0.9520 - val_loss: 0.4362 - val_sparse_categorical_accuracy: 0.8620\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1730 - sparse_categorical_accuracy: 0.9640 - val_loss: 0.4403 - val_sparse_categorical_accuracy: 0.8680\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1587 - sparse_categorical_accuracy: 0.9680 - val_loss: 0.4106 - val_sparse_categorical_accuracy: 0.8680\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1289 - sparse_categorical_accuracy: 0.9790 - val_loss: 0.4180 - val_sparse_categorical_accuracy: 0.8610\n"
     ]
    }
   ],
   "source": [
    "h2 = model2.fit(x=train_images2, y=train_labels2, batch_size=128, epochs=10,\n",
    "            validation_data=(test_images2, test_labels2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 1.7301 - sparse_categorical_accuracy: 0.5240 - val_loss: 1.1793 - val_sparse_categorical_accuracy: 0.7300\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.7802 - sparse_categorical_accuracy: 0.8340 - val_loss: 0.7229 - val_sparse_categorical_accuracy: 0.8090\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.4612 - sparse_categorical_accuracy: 0.8880 - val_loss: 0.5955 - val_sparse_categorical_accuracy: 0.8120\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.3324 - sparse_categorical_accuracy: 0.9060 - val_loss: 0.5227 - val_sparse_categorical_accuracy: 0.8440\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.2524 - sparse_categorical_accuracy: 0.9390 - val_loss: 0.5024 - val_sparse_categorical_accuracy: 0.8470\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.2084 - sparse_categorical_accuracy: 0.9520 - val_loss: 0.4847 - val_sparse_categorical_accuracy: 0.8560\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1621 - sparse_categorical_accuracy: 0.9720 - val_loss: 0.4710 - val_sparse_categorical_accuracy: 0.8610\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1393 - sparse_categorical_accuracy: 0.9760 - val_loss: 0.4670 - val_sparse_categorical_accuracy: 0.8580\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1208 - sparse_categorical_accuracy: 0.9790 - val_loss: 0.4563 - val_sparse_categorical_accuracy: 0.8580\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1031 - sparse_categorical_accuracy: 0.9890 - val_loss: 0.4526 - val_sparse_categorical_accuracy: 0.8630\n"
     ]
    }
   ],
   "source": [
    "h3 = model3.fit(x=train_images3, y=train_labels3, batch_size=128, epochs=10, \n",
    "                        validation_data=(test_images3, test_labels3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "histList = get_loss_or_acc([h1, h2, h3], loss=\"val_loss\")\n",
    "histList2 = get_loss_or_acc([h1,h2,h3], acc=\"val_sparse_categorical_accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.41858091950416565, 0.4146658778190613, 0.45149677991867065]"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "histList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.8820000290870667, 0.8619999885559082, 0.8610000014305115]"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "histList2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "# Trying to get the new weights for the models\n",
    "w1 = model1.get_weights()\n",
    "w2 = model2.get_weights()\n",
    "w3 = model3.get_weights()\n",
    "type(w1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"functional_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 784)]             0         \n_________________________________________________________________\ndense (Dense)                (None, 512)               401920    \n_________________________________________________________________\ndropout (Dropout)            (None, 512)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 10)                5130      \n=================================================================\nTotal params: 407,050\nTrainable params: 407,050\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The type of j is <class 'numpy.ndarray'>\nThe shape of j is (784, 512)\nThe type of j is <class 'numpy.ndarray'>\nThe shape of j is (512,)\nThe type of j is <class 'numpy.ndarray'>\nThe shape of j is (512, 10)\nThe type of j is <class 'numpy.ndarray'>\nThe shape of j is (10,)\n"
     ]
    }
   ],
   "source": [
    "# looping through the weights to see what it looks like\n",
    "for j in w1:\n",
    "    print(f\"The type of j is {type(j)}\")\n",
    "    if isinstance(j, np.ndarray):\n",
    "        print(f\"The shape of j is {j.shape}\")\n",
    "    if isinstance(j, list):\n",
    "        print(f\"The lenght of j is {len(j)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the new weights\n",
    "new_weights_from_loss = create_weight_avg(allWeights=[w1, w2, w3], amount=.001, loss=histList)\n",
    "new_weights_from_acc = create_weight_avg(allWeights=[w1, w2, w3], acc=histList2, amount=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(list, 4, (784, 512))"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "type(new_weights_from_loss) ,len(new_weights_from_loss), new_weights_from_loss[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(list, 4, (784, 512))"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "# comparing to the first weights\n",
    "type(w1), len(w1), w1[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a totally new model\n",
    "loss_model = create_model()\n",
    "loss_model.set_weights(new_weights_from_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_model = create_model()\n",
    "acc_model.set_weights(new_weights_from_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running through all the data\n",
    "test_images, test_labels = reshape_data(test, start_index=0, end_index=len(test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.5295 - sparse_categorical_accuracy: 0.8377\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.529538094997406, 0.8377000093460083]"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "model1.evaluate(x=test_images, y=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3803 - sparse_categorical_accuracy: 0.8832\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.380273699760437, 0.8831999897956848]"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "model2.evaluate(x=test_images, y=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3912 - sparse_categorical_accuracy: 0.8796\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.3911978304386139, 0.8795999884605408]"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "model3.evaluate(x=test_images, y=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.9042 - sparse_categorical_accuracy: 0.8217\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.9041731357574463, 0.8216999769210815]"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "# doing the evaluation of the models \n",
    "loss_model.evaluate(x=test_images, y=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.9040 - sparse_categorical_accuracy: 0.8222\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.9039875268936157, 0.8222000002861023]"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "# doing the evaluation of the model where we looked at the best accuracy\n",
    "acc_model.evaluate(x=test_images, y=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking to see what the accuracy will be with the model trained on all the \n",
    "# data.\n",
    "all_data_model = create_model()\n",
    "# getting the data for the model\n",
    "train, test = load_images()\n",
    "# getting the data in the correct format\n",
    "train_images, train_labels = reshape_data(train, start_index=0, end_index=len(train[0]))\n",
    "test_images , test_labels = reshape_data(test, start_index=0, end_index=len(test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a callback for the tensorboard\n",
    "the_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# creating the tensorboard callback\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=the_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "  2/469 [..............................] - ETA: 1:31 - loss: 0.0021 - sparse_categorical_accuracy: 1.0000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0050s vs `on_train_batch_end` time: 0.3873s). Check your callbacks.\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.0060 - sparse_categorical_accuracy: 0.9980 - val_loss: 0.0928 - val_sparse_categorical_accuracy: 0.9825\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0035 - sparse_categorical_accuracy: 0.9988 - val_loss: 0.0902 - val_sparse_categorical_accuracy: 0.9840\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0048 - sparse_categorical_accuracy: 0.9983 - val_loss: 0.0960 - val_sparse_categorical_accuracy: 0.9837\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0043 - sparse_categorical_accuracy: 0.9984 - val_loss: 0.0825 - val_sparse_categorical_accuracy: 0.9842\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0056 - sparse_categorical_accuracy: 0.9978 - val_loss: 0.0902 - val_sparse_categorical_accuracy: 0.9826\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0033 - sparse_categorical_accuracy: 0.9988 - val_loss: 0.0903 - val_sparse_categorical_accuracy: 0.9840\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0052 - sparse_categorical_accuracy: 0.9982 - val_loss: 0.0950 - val_sparse_categorical_accuracy: 0.9844\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0040 - sparse_categorical_accuracy: 0.9987 - val_loss: 0.0965 - val_sparse_categorical_accuracy: 0.9829\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0045 - sparse_categorical_accuracy: 0.9984 - val_loss: 0.0882 - val_sparse_categorical_accuracy: 0.9840\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0040 - sparse_categorical_accuracy: 0.9988 - val_loss: 0.0971 - val_sparse_categorical_accuracy: 0.9830\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0971 - sparse_categorical_accuracy: 0.9830\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.09707936644554138, 0.9829999804496765]"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "# now fitting the model and then will evaluate with the test data\n",
    "all_data_model.fit(x=train_images, y=train_labels, batch_size=128, epochs=10, validation_data=(test_images, test_labels), \n",
    "                    callbacks=[tensorboard_callback])\n",
    "all_data_model.evaluate(x=test_images, y=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4\n784 512 512 10\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((784, 512), (512,), (512, 10), (10,))"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "w = all_data_model.get_weights()\n",
    "print(len(w))\n",
    "print(len(w[0]), len(w[1]), len(w[2]), len(w[3]))\n",
    "w[0].shape, w[1].shape, w[2].shape,   w[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am going to try the three models again, but this time I will \n",
    "# mix in the data so that some of the data is seen by more than one model\n",
    "\n",
    "# splitting up the data to some chunks\n",
    "\n",
    "# loading the data of the mnist\n",
    "train, test = load_images()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping the data\n",
    "train_img, train_label = reshape_data(train, start_index=0, end_index=len(train[0]))\n",
    "test_img, test_label = reshape_data(test, start_index=0, end_index=len(test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_t = (train_img, train_label)\n",
    "test_t= (test_img, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The estimated data_chunk size will be aproximately ---  17025\nThe estimated size of the original data per data_chunk (window) ---  14325\nThe estimated size of data that is found in all (in_all) the data chunks --- 2700\nThe estimated number of trunks made will be --- 4\n"
     ]
    }
   ],
   "source": [
    "# checking the sizes of the chunks \n",
    "chunk_size(train_t, data_chunk=.25, in_all=.18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "These are the real values for the sizes for the data chunks\n",
      "The size of data in all chunks is 2704\n",
      "The size of the original data in each chunk is 14324\n",
      "\n",
      "These are the real values for the sizes for the data chunks\n",
      "The size of data in all chunks is 376\n",
      "The size of the original data in each chunk is 2406\n"
     ]
    }
   ],
   "source": [
    "# doing the chunking of the data\n",
    "train_list = chunk_shuffle(data=train_t, data_chunk=.25,in_all=.18,rand_seed=49 )\n",
    "test_list = chunk_shuffle(data=test_t, data_chunk=.25, in_all=.15, rand_seed=49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "These are the sizes of the data for the train\nThe size of the number 1 train image is (21562, 784)\nThe size of the number 1 train label is (21562,)\nThe size of the number 2 train image is (21562, 784)\nThe size of the number 2 train label is (21562,)\nThe size of the number 3 train image is (23322, 784)\nThe size of the number 3 train label is (23322,)\nThe size of the number 1 test image is (3594, 784)\nThe size of the number 1 test label is (3594,)\nThe size of the number 2 test image is (3594, 784)\nThe size of the number 2 test label is (3594,)\nThe size of the number 3 test image is (3859, 784)\nThe size of the number 3 test label is (3859,)\n"
     ]
    }
   ],
   "source": [
    "# checking to see what the sizes of all the data is\n",
    "print(\"These are the sizes of the data for the train\")\n",
    "the_count = 1\n",
    "for chunk in train_list:\n",
    "    print(f\"The size of the number {the_count} train image is {chunk[0].shape}\")\n",
    "    print(f\"The size of the number {the_count} train label is {chunk[1].shape}\")\n",
    "    the_count +=1\n",
    "the_count = 1\n",
    "for chunk in test_list:\n",
    "    print(f\"The size of the number {the_count} test image is {chunk[0].shape}\")\n",
    "    print(f\"The size of the number {the_count} test label is {chunk[1].shape}\")\n",
    "    the_count +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the models for three blocks\n",
    "model1 = create_model()\n",
    "model2 =create_model()\n",
    "model3 = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "134/134 [==============================] - 1s 7ms/step - loss: 0.0129 - sparse_categorical_accuracy: 0.9979 - val_loss: 0.0997 - val_sparse_categorical_accuracy: 0.9722\n",
      "Epoch 2/5\n",
      "134/134 [==============================] - 1s 7ms/step - loss: 0.0104 - sparse_categorical_accuracy: 0.9982 - val_loss: 0.1039 - val_sparse_categorical_accuracy: 0.9718\n",
      "Epoch 3/5\n",
      "134/134 [==============================] - 1s 6ms/step - loss: 0.0099 - sparse_categorical_accuracy: 0.9987 - val_loss: 0.1044 - val_sparse_categorical_accuracy: 0.9700\n",
      "Epoch 4/5\n",
      "134/134 [==============================] - 1s 6ms/step - loss: 0.0104 - sparse_categorical_accuracy: 0.9980 - val_loss: 0.1057 - val_sparse_categorical_accuracy: 0.9716\n",
      "Epoch 5/5\n",
      "134/134 [==============================] - 1s 7ms/step - loss: 0.0129 - sparse_categorical_accuracy: 0.9969 - val_loss: 0.1027 - val_sparse_categorical_accuracy: 0.9727\n"
     ]
    }
   ],
   "source": [
    "# doing the fitting of the models \n",
    "hist1 = model1.fit(x=train_list[0][0], y=train_list[0][1], batch_size=128, epochs=5, validation_data=(test_img, test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "134/134 [==============================] - 1s 7ms/step - loss: 0.0137 - sparse_categorical_accuracy: 0.9974 - val_loss: 0.1042 - val_sparse_categorical_accuracy: 0.9697\n",
      "Epoch 2/5\n",
      "134/134 [==============================] - 1s 6ms/step - loss: 0.0115 - sparse_categorical_accuracy: 0.9981 - val_loss: 0.0998 - val_sparse_categorical_accuracy: 0.9722\n",
      "Epoch 3/5\n",
      "134/134 [==============================] - 1s 6ms/step - loss: 0.0102 - sparse_categorical_accuracy: 0.9984 - val_loss: 0.0997 - val_sparse_categorical_accuracy: 0.9720\n",
      "Epoch 4/5\n",
      "134/134 [==============================] - 1s 7ms/step - loss: 0.0089 - sparse_categorical_accuracy: 0.9985 - val_loss: 0.1022 - val_sparse_categorical_accuracy: 0.9723\n",
      "Epoch 5/5\n",
      "134/134 [==============================] - 1s 6ms/step - loss: 0.0087 - sparse_categorical_accuracy: 0.9987 - val_loss: 0.0998 - val_sparse_categorical_accuracy: 0.9720\n"
     ]
    }
   ],
   "source": [
    "hist2 = model2.fit(x=train_list[1][0], y=train_list[1][1], batch_size=128, epochs=5, validation_data=(test_img, test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "134/134 [==============================] - 1s 7ms/step - loss: 0.0301 - sparse_categorical_accuracy: 0.9912 - val_loss: 0.1021 - val_sparse_categorical_accuracy: 0.9699\n",
      "Epoch 2/5\n",
      "134/134 [==============================] - 1s 7ms/step - loss: 0.0179 - sparse_categorical_accuracy: 0.9960 - val_loss: 0.1026 - val_sparse_categorical_accuracy: 0.9714\n",
      "Epoch 3/5\n",
      "134/134 [==============================] - 1s 7ms/step - loss: 0.0121 - sparse_categorical_accuracy: 0.9978 - val_loss: 0.1021 - val_sparse_categorical_accuracy: 0.9719\n",
      "Epoch 4/5\n",
      "134/134 [==============================] - 1s 6ms/step - loss: 0.0114 - sparse_categorical_accuracy: 0.9982 - val_loss: 0.1061 - val_sparse_categorical_accuracy: 0.9717\n",
      "Epoch 5/5\n",
      "134/134 [==============================] - 1s 6ms/step - loss: 0.0175 - sparse_categorical_accuracy: 0.9954 - val_loss: 0.1042 - val_sparse_categorical_accuracy: 0.9720\n"
     ]
    }
   ],
   "source": [
    "hist3 = model3.fit(x=train_list[2][0], y=train_list[2][1], batch_size=128, epochs=5, validation_data=(test_img, test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "histList = get_loss_or_acc([hist1, hist2, hist3], loss=\"val_loss\")\n",
    "histList2 = get_loss_or_acc([hist1,hist2,hist3], acc=\"val_sparse_categorical_accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = model1.get_weights()\n",
    "w2 = model2.get_weights()\n",
    "w3 = model3.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the new weights\n",
    "new_weights_from_loss = create_weight_avg(allWeights=[w1, w2, w3], amount=.001, loss=histList)\n",
    "new_weights_from_acc = create_weight_avg(allWeights=[w1, w2, w3], acc=histList2, amount=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making two different models to use\n",
    "loss_model = create_model()\n",
    "acc_model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the weights of the model\n",
    "loss_model.set_weights(new_weights_from_loss)\n",
    "acc_model.set_weights(new_weights_from_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3887 - sparse_categorical_accuracy: 0.9463\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.38873612880706787, 0.9463000297546387]"
      ]
     },
     "metadata": {},
     "execution_count": 94
    }
   ],
   "source": [
    "# doing the evaluation of each of the models\n",
    "loss_model.evaluate(x=test_img, y=test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3888 - sparse_categorical_accuracy: 0.9465\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.3887940049171448, 0.9465000033378601]"
      ]
     },
     "metadata": {},
     "execution_count": 95
    }
   ],
   "source": [
    "acc_model.evaluate(x=test_img, y = test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "469/469 [==============================] - 3s 7ms/step - loss: 0.2730 - sparse_categorical_accuracy: 0.9286 - val_loss: 0.1363 - val_sparse_categorical_accuracy: 0.9580\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24901e823d0>"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "# will now train again the model\n",
    "loss_model.fit(x=train_img,y=train_label, batch_size=128, epochs=1, validation_data=(test_img, test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}