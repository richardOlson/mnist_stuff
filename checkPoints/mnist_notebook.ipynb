{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "tensorflow_env",
   "display_name": "tensorflow_env",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import os\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "# importing the operator module to do some comparisons\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"\n",
    "    The function to create a model\n",
    "    \"\"\"\n",
    "    inputs = keras.layers.Input(shape=(784,))\n",
    "    t = keras.layers.Dense(512, activation=\"relu\", )(inputs)\n",
    "    t = keras.layers.Dropout(.2)(t)\n",
    "    out = keras.layers.Dense(10, )(t)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=out)\n",
    "    # doing the compilation of the model\n",
    "    model.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the model\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'.\\\\..\\\\checkPoints/cp.ckpt'"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "# getting the path\n",
    "path = os.path.join(os.path.curdir, \"..\", \"checkPoints/cp.ckpt\")\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x255ce3cab20>"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "model.load_weights(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"functional_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         [(None, 784)]             0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 512)               401920    \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 512)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 10)                5130      \n=================================================================\nTotal params: 407,050\nTrainable params: 407,050\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# showing the summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The shape of what is returned with the weigths is ---- 0\nThe type is --- <class 'list'>\nThese are the weights for the input_1 layer --- []\nThe shape of what is returned with the weigths is ---- 2\nThe type is --- <class 'list'>\nThe type for each item in the list is: <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\nThese are the weights for the dense layer --- [<tf.Variable 'dense/kernel:0' shape=(784, 512) dtype=float32, numpy=\narray([[-0.05626954, -0.05369684, -0.06167276, ..., -0.001957  ,\n         0.00331007,  0.05483547],\n       [-0.049044  , -0.04101725,  0.04828054, ...,  0.0366065 ,\n         0.00066533,  0.05718373],\n       [ 0.00024945,  0.05019872, -0.0286467 , ...,  0.05689124,\n        -0.01853205,  0.05268286],\n       ...,\n       [-0.02224023, -0.06074744,  0.06550466, ..., -0.06264395,\n        -0.02794588, -0.01869341],\n       [ 0.03836417, -0.06636625, -0.0272741 , ...,  0.06481206,\n         0.06491137,  0.03532643],\n       [-0.02355461, -0.01153382,  0.01450439, ..., -0.02485488,\n        -0.00436073,  0.04444816]], dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(512,) dtype=float32, numpy=\narray([ 1.72597487e-04,  1.05121862e-02, -9.93606634e-03,  1.65048260e-02,\n        1.48517517e-02,  1.02724703e-02,  2.61683874e-02,  7.80888041e-03,\n       -6.67838706e-03, -2.63294880e-03,  7.54132587e-03,  1.07546821e-02,\n       -3.55655327e-04,  4.24108701e-03,  6.80069160e-03,  9.35562234e-03,\n       -1.48517676e-02, -1.26193035e-02,  2.14740597e-02,  1.90498773e-02,\n        1.75802875e-02,  1.00164497e-02,  2.33117156e-02,  5.82871586e-03,\n        1.24599226e-02,  2.49987701e-03, -4.81321290e-03, -3.32437549e-03,\n        2.52118185e-02,  1.74722690e-02,  5.97302278e-04,  5.02793957e-03,\n        1.51756210e-02,  8.94126110e-03,  1.02106249e-02,  1.77973378e-02,\n        1.91553961e-02, -4.73733991e-03,  6.50379900e-03, -1.48366541e-02,\n        9.10403393e-03,  5.40261390e-03,  1.62270386e-02,  1.17752440e-02,\n       -5.23779681e-03, -6.07746793e-03,  5.50066493e-03,  3.80394142e-03,\n        1.03695886e-02,  1.43970409e-02,  9.94092785e-03,  9.76029492e-04,\n       -7.81322550e-03, -2.01762130e-04,  3.46075930e-03,  1.20185986e-02,\n       -1.87506294e-03,  6.11349475e-03,  1.68360770e-02,  1.68225840e-02,\n       -2.46017892e-03, -1.42869249e-03,  1.85877215e-02,  1.65404603e-02,\n        2.38595940e-02,  1.01817586e-02,  2.15113480e-02,  1.31973531e-02,\n       -5.73000079e-03,  1.58557054e-02, -4.49281745e-03,  2.04001972e-03,\n        4.20698710e-03,  1.88715048e-02,  4.06323420e-03, -3.21356405e-04,\n        1.24803148e-02,  1.13657508e-02, -1.42590879e-02, -6.26258180e-03,\n        8.94737802e-03,  1.17486324e-02, -1.33981407e-02,  6.00477168e-03,\n        3.67667386e-03,  1.21088652e-02,  1.48060801e-03,  1.78699940e-02,\n        1.67217135e-04,  4.69582807e-03,  1.36005282e-02,  2.41139308e-02,\n       -1.46972360e-02,  1.36002703e-02,  8.92073568e-03,  2.23400593e-02,\n        5.55964792e-03,  1.95225626e-02,  1.62121840e-02,  3.65433225e-04,\n        2.03223117e-02,  9.67860874e-03, -6.90692523e-03,  1.18965907e-02,\n        1.19682113e-02, -4.15906496e-03,  8.81340820e-03, -1.83345713e-02,\n        1.10727884e-02,  1.21528478e-02, -1.50002241e-02,  1.79905035e-02,\n        1.85826402e-02, -3.98032740e-03, -4.49450081e-03,  1.26722511e-02,\n        1.90368434e-03,  3.97697091e-03, -1.27569510e-04,  7.51278829e-03,\n       -4.52209311e-03,  8.58710706e-03,  1.38698379e-02, -2.46764021e-03,\n        1.09259719e-02, -5.99331968e-03, -6.49669115e-03,  2.59255469e-02,\n        1.07646296e-02, -9.94778238e-04,  1.35442580e-03,  6.43221429e-03,\n        1.55213065e-02, -1.03907716e-02,  1.67679079e-02,  1.48895886e-02,\n       -1.18476804e-02,  1.07257171e-02,  3.55914212e-03,  1.11360867e-02,\n        2.93036830e-03, -3.82516207e-03, -7.80127384e-03, -2.39326619e-03,\n        2.46371757e-02, -1.27290459e-02,  4.07224707e-03,  2.12547034e-02,\n        1.63449980e-02, -1.96036268e-02,  1.01368725e-02,  5.91298752e-03,\n       -5.23868762e-03,  1.61788203e-02, -4.67349915e-03,  1.06823221e-02,\n        5.90169919e-04, -6.95585785e-03,  1.34356255e-02,  9.71553940e-03,\n        1.97174270e-02,  1.70956999e-02,  1.06709301e-02, -6.54631713e-03,\n       -7.34526431e-04,  1.74862556e-02,  1.42350337e-02, -6.98204851e-04,\n        1.40222372e-03,  9.99701302e-03, -5.94796031e-04, -3.38633684e-03,\n        6.83624577e-03,  2.06210911e-02,  6.89667277e-03, -9.13625769e-03,\n        2.23413692e-03,  1.46558462e-02, -1.52699575e-02,  1.26199508e-02,\n       -3.74557939e-03,  1.70344543e-02,  1.93049088e-02, -9.92825907e-03,\n        1.24239745e-02,  1.85128339e-02, -1.59667118e-03,  1.87675506e-02,\n        1.22579010e-02,  2.29209736e-02,  1.78051554e-02,  4.82282881e-03,\n        1.65875424e-02,  1.39386905e-02,  1.37819564e-02, -6.30658527e-04,\n       -9.30631813e-03,  1.08812954e-02,  1.87493283e-02,  1.71661433e-02,\n        1.36542949e-03,  3.62640317e-03,  8.66733212e-03,  2.22192612e-02,\n       -9.54966247e-03,  2.52975114e-02, -6.86078239e-03,  3.23635247e-03,\n        2.44395831e-03,  1.17151979e-02,  5.28266234e-03,  4.39155614e-03,\n        6.83129486e-03,  2.26570442e-02,  1.96919180e-02, -5.14318841e-03,\n       -1.46707119e-02,  1.50537351e-02, -7.96610583e-03, -5.84941404e-03,\n       -1.04225511e-02, -3.48156667e-03,  6.20870618e-03,  8.47083516e-04,\n       -3.22766340e-04,  2.02458296e-02,  8.79207999e-03, -1.08482623e-02,\n       -4.10871720e-03,  1.82971377e-02,  1.92662328e-02,  2.38690572e-03,\n       -3.01049603e-03, -7.71063007e-03,  7.57430261e-03,  5.64594427e-03,\n        4.10549762e-03,  1.92663120e-03, -3.68727138e-03, -1.50720710e-02,\n        4.26016469e-03,  8.37763958e-03,  1.87635794e-02,  1.72189120e-02,\n        1.94656663e-02,  7.15053687e-03,  8.85615405e-03, -1.42346381e-03,\n       -9.99983400e-03,  2.16865949e-02, -1.71931449e-03,  3.17133451e-03,\n        3.99380783e-03,  1.99397691e-02,  2.40188669e-02,  1.31260483e-02,\n        3.68883112e-03,  1.27733145e-02, -3.24683590e-03, -3.65647790e-03,\n        2.10799966e-02,  1.31309489e-02,  1.81905795e-02,  5.53172082e-04,\n        1.09740598e-02,  1.67485494e-02,  1.38644679e-02,  2.56677456e-02,\n       -5.92459692e-03, -9.01204161e-03,  1.50966719e-02,  1.90052949e-02,\n        5.04563795e-03,  1.95423011e-02, -2.22481205e-03,  2.00304314e-02,\n       -2.42300448e-03,  1.94869041e-02,  1.60062574e-02,  1.85983982e-02,\n        1.06929727e-02, -7.58236181e-03,  2.66808104e-02, -1.34435343e-02,\n        1.13502573e-02,  2.58019683e-03, -9.92083363e-03, -1.13562159e-02,\n        2.50601186e-03,  1.24915186e-02,  1.46082081e-02,  9.80041642e-03,\n        2.44212858e-02,  4.38334094e-03,  2.25721393e-03,  2.59431899e-02,\n        1.03705528e-03,  5.60802687e-03,  2.73208413e-02,  1.78735913e-03,\n       -7.89626210e-05,  5.90488361e-03,  9.59941931e-03,  7.09745847e-03,\n        5.42883063e-03,  1.11090923e-02, -1.65096018e-03,  9.30715818e-03,\n        1.25081968e-02,  5.28563047e-03,  1.38063738e-02,  1.55692799e-02,\n        1.94110665e-02,  1.20160030e-02,  1.19931754e-02, -9.86145250e-03,\n       -4.20161895e-03,  1.11806933e-02, -1.16814449e-02,  1.95057283e-03,\n       -1.19410164e-03, -5.52692974e-04,  1.86904371e-02,  7.18024885e-03,\n        2.42474792e-03,  5.96098136e-04,  2.52153887e-03,  2.20747739e-02,\n        2.95085032e-02,  1.31511874e-02,  2.25789733e-02, -1.11722788e-04,\n        1.16808983e-02,  1.92412268e-02,  9.21589043e-03,  3.73866945e-03,\n        8.38876050e-03,  2.29975265e-02,  1.25467060e-02, -8.58375989e-03,\n        1.19069275e-02,  1.16868911e-03, -3.79161863e-03,  2.32451450e-04,\n        1.03091856e-03,  1.29203321e-02, -4.12527053e-03,  8.03037488e-04,\n        6.71883160e-03,  3.42380581e-03,  7.29215425e-03, -5.46654360e-03,\n        5.38758375e-03, -4.01928835e-03,  1.97195783e-02, -1.42078269e-02,\n        1.46365268e-02,  1.39304148e-02, -7.20077660e-05, -3.99321085e-03,\n        1.80188045e-02,  1.18029630e-02, -5.73757617e-03, -1.67114806e-04,\n        2.41491292e-03, -6.95342082e-04,  2.10326128e-02,  7.40283588e-03,\n       -1.23865912e-02,  8.13089591e-03,  3.47751984e-03,  1.52790127e-02,\n        1.62633304e-02,  6.99118851e-03,  1.67725086e-02,  2.04010010e-02,\n        7.17896549e-03, -6.63141068e-03, -2.67516077e-03, -3.48725333e-03,\n       -1.08487699e-02, -1.05346544e-02, -1.14916568e-03, -4.90661850e-03,\n        2.67180521e-03,  2.25566644e-02,  7.95148686e-03,  8.24715383e-03,\n       -6.75756112e-03, -6.13528304e-03,  1.26109244e-02,  1.65690240e-02,\n       -9.37484019e-03,  1.85696762e-02,  3.04171462e-02,  8.15941207e-03,\n        1.68669783e-02,  1.80480856e-04,  1.92198344e-02, -1.38751874e-02,\n        2.82430113e-03,  7.58931227e-03, -5.92449505e-05,  1.37020685e-02,\n        1.05779839e-03,  1.03384741e-02,  1.86582841e-02,  1.13291712e-02,\n       -2.57410631e-02,  6.47610752e-03,  2.68482175e-02,  2.21051555e-03,\n        3.20901396e-03,  7.53921317e-03, -7.31907692e-03,  1.43468492e-02,\n        2.40712725e-02,  5.22164768e-03,  8.10789689e-03,  1.42931417e-02,\n       -1.13822753e-02,  4.85085184e-03,  3.26441508e-03,  5.15708118e-04,\n        1.23437187e-02,  2.07592417e-02,  2.08405964e-03,  2.08609216e-02,\n       -7.50211300e-03,  2.43883003e-02,  1.71769746e-02,  3.45202279e-04,\n        7.74060842e-03,  1.75596327e-02,  1.49875330e-02,  1.00264782e-02,\n        2.27080137e-02, -4.31349734e-03, -8.22057715e-04, -5.30624529e-03,\n        1.44437999e-02,  1.01032453e-02,  3.02405711e-02,  1.30343810e-02,\n       -9.96138249e-03, -1.17465463e-02,  6.86999783e-03,  1.69459078e-02,\n        8.81494116e-03,  1.89694893e-02, -8.15362297e-03,  2.04268526e-02,\n        2.05217451e-02,  2.41035428e-02,  1.63802225e-02,  2.44297888e-02,\n        6.81259716e-03, -4.27511724e-04, -8.88006855e-03,  1.69149449e-03,\n       -1.00014545e-02, -2.10044812e-03,  2.84275389e-03,  5.16199833e-03,\n       -2.96031102e-03, -9.93147027e-03,  9.39159933e-03, -7.03254715e-04,\n       -5.56486985e-03,  1.94572415e-02,  5.54063357e-04,  1.27571467e-02,\n        6.32402534e-03,  1.54216932e-02,  2.88840686e-03,  4.95937327e-03,\n        1.87297799e-02, -1.37654459e-02,  3.01342495e-02,  1.98066961e-02,\n        4.67774924e-03,  1.97523683e-02,  3.84773174e-03,  1.51203284e-02,\n        1.42727140e-02,  2.30135377e-02, -1.43146319e-02,  1.01353768e-02,\n       -7.84549024e-03,  1.54151777e-02, -2.93102057e-04,  3.77435004e-03,\n        2.01666355e-02,  8.91412981e-03,  1.88908970e-03,  1.84706245e-02,\n       -1.27008101e-02,  1.77656468e-02,  4.96036559e-03,  6.27404312e-03,\n        2.41043326e-02, -9.00016259e-03,  2.04670690e-02,  4.75749187e-03,\n        1.76593731e-03,  1.06164468e-02,  1.41866617e-02, -2.15561278e-02,\n        4.11901297e-03, -1.51740536e-02,  9.53926146e-03,  2.51788497e-02],\n      dtype=float32)>]\nThe shape of what is returned with the weigths is ---- 0\nThe type is --- <class 'list'>\nThese are the weights for the dropout layer --- []\nThe shape of what is returned with the weigths is ---- 2\nThe type is --- <class 'list'>\nThe type for each item in the list is: <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\nThese are the weights for the dense_1 layer --- [<tf.Variable 'dense_1/kernel:0' shape=(512, 10) dtype=float32, numpy=\narray([[ 0.03622122, -0.03641613,  0.12541647, ..., -0.03731978,\n        -0.03972755, -0.00030406],\n       [-0.09065956, -0.04421225, -0.00835247, ..., -0.06340674,\n        -0.14152284, -0.06728777],\n       [ 0.02464028, -0.04258672,  0.04578171, ..., -0.04067839,\n         0.03545604, -0.0039114 ],\n       ...,\n       [ 0.04221679,  0.08953761,  0.11619351, ...,  0.02745421,\n        -0.02400982, -0.07182321],\n       [-0.09616829, -0.06718431,  0.09197892, ..., -0.07621801,\n        -0.06656138, -0.13127476],\n       [ 0.00644363,  0.08017394,  0.00601754, ...,  0.12485438,\n        -0.04525638, -0.0403614 ]], dtype=float32)>, <tf.Variable 'dense_1/bias:0' shape=(10,) dtype=float32, numpy=\narray([-0.02103808,  0.00912677, -0.00462178, -0.01466964,  0.01683323,\n        0.01430766, -0.00607732,  0.01283323, -0.01082663, -0.00280234],\n      dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "# printing the weights of the layers\n",
    "counter = 0\n",
    "for layer in model.layers:\n",
    "    print(f\"The shape of what is returned with the weigths is ---- {len(layer.weights)}\")\n",
    "    print(f\"The type is --- {type(layer.weights)}\")\n",
    "    if len(layer.weights) > 0:\n",
    "        print(f\"The type for each item in the list is: {type(layer.weights[0])}\")\n",
    "    print(f\"These are the weights for the {layer.name} layer --- {layer.weights}\" )\n",
    "    counter += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the data for another block  that is different from the one that the modle above is trained with.\n",
    "def load_images():\n",
    "    training, testing = keras.datasets.mnist.load_data()\n",
    "    # Each of these are tuples that contain ndarrays\n",
    "\n",
    "    return training, testing\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def reshape_data(data, start_index:int, end_index:int ):\n",
    "    images = None\n",
    "    labels = None\n",
    "    if isinstance(data, tuple):\n",
    "        images, labels = data\n",
    "    \n",
    "    # made the slice to pull from the opposite end\n",
    "    images = images[start_index: end_index].reshape(-1, (28 * 28))\n",
    "    labels = labels[start_index: end_index]\n",
    "    \n",
    "    images = images/255.0\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((1000, 784), (1000,))"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# now creating a model\n",
    "# looking at the shape of the train \n",
    "train_images.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 1.5915 - sparse_categorical_accuracy: 0.5840 - val_loss: 0.9791 - val_sparse_categorical_accuracy: 0.8110\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.5774 - sparse_categorical_accuracy: 0.8940 - val_loss: 0.6365 - val_sparse_categorical_accuracy: 0.8230\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.3004 - sparse_categorical_accuracy: 0.9370 - val_loss: 0.5017 - val_sparse_categorical_accuracy: 0.8500\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.2014 - sparse_categorical_accuracy: 0.9550 - val_loss: 0.4593 - val_sparse_categorical_accuracy: 0.8700\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1432 - sparse_categorical_accuracy: 0.9710 - val_loss: 0.4497 - val_sparse_categorical_accuracy: 0.8640\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1147 - sparse_categorical_accuracy: 0.9780 - val_loss: 0.4310 - val_sparse_categorical_accuracy: 0.8740\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0952 - sparse_categorical_accuracy: 0.9840 - val_loss: 0.4405 - val_sparse_categorical_accuracy: 0.8710\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0772 - sparse_categorical_accuracy: 0.9910 - val_loss: 0.4264 - val_sparse_categorical_accuracy: 0.8850\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0632 - sparse_categorical_accuracy: 0.9920 - val_loss: 0.4189 - val_sparse_categorical_accuracy: 0.8840\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0535 - sparse_categorical_accuracy: 0.9940 - val_loss: 0.4269 - val_sparse_categorical_accuracy: 0.8770\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x255cdd512e0>"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "model2.fit(x=train_images, y=train_labels, batch_size=128, epochs=10, \n",
    "            validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(list, tensorflow.python.ops.resource_variable_ops.ResourceVariable)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# getting the weights for this model\n",
    "the_weights = model2.weights\n",
    "type(the_weights), type(the_weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Variable 'dense/kernel:0' shape=(784, 512) dtype=float32, numpy=\n",
       "array([[ 2.0674616e-02,  3.2345042e-02, -7.7739358e-05, ...,\n",
       "        -6.2627956e-02, -3.8801931e-02, -1.3471089e-02],\n",
       "       [ 2.7597107e-02,  5.6585222e-02,  1.0034814e-03, ...,\n",
       "         3.5107583e-03, -3.3186216e-02, -1.6239803e-02],\n",
       "       [ 2.3986742e-02,  2.8475918e-02,  1.3272785e-02, ...,\n",
       "        -1.3933085e-02,  1.9999221e-03,  1.2072645e-02],\n",
       "       ...,\n",
       "       [ 8.9056566e-03,  3.4107566e-02, -5.5254426e-02, ...,\n",
       "        -5.1518083e-03,  1.5794642e-02,  3.3432893e-02],\n",
       "       [-2.1421652e-02,  2.4194144e-02,  2.5040790e-02, ...,\n",
       "        -2.1030013e-02,  6.0876384e-02, -6.0057208e-02],\n",
       "       [-3.4105852e-03,  2.0341501e-03,  6.6730469e-02, ...,\n",
       "        -4.5968406e-02,  2.5523305e-02,  5.9543997e-02]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "the_weights[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(numpy.ndarray, (784, 512))"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# getting the weights of the model\n",
    "w = the_weights[0].numpy()\n",
    "type(w), w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(numpy.ndarray, (784, 512))"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "# getting the weights of the first model above\n",
    "first_w = model.weights[0].numpy()\n",
    "type(first_w), w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "# getting the average of the two weights\n",
    "avg_wt = (w + first_w)/2\n",
    "type(avg_wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'list'> <class 'numpy.ndarray'> 4\n-first is (784, 512), second (512,), third (512, 10), 4th is (10,)\n"
     ]
    }
   ],
   "source": [
    "# getting the item of the tensorflow variables\n",
    "the_var_weights = model2.get_weights()\n",
    "print(type(the_var_weights),  type(the_var_weights[0]), len(the_var_weights))\n",
    "print(f\"-first is {the_var_weights[0].shape}, second {the_var_weights[1].shape}, third {the_var_weights[2].shape}, 4th is {the_var_weights[3].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will find the average using the loss or the acc \n",
    "# If using the loss the best one is the one with the least loss \n",
    "# If using the acc then the best one is the one with the highest acc\n",
    "def get_avg_with_metric(listArr:list, amount:float, loss=None, acc=None ):\n",
    "    best_val = None\n",
    "    best_arr = 0\n",
    "    metric = None\n",
    "    \n",
    "    # this is the number to divide by to get the average\n",
    "    divide_for_avg = 0\n",
    "    # array that will hold all the values and will hold the end result of the avg \n",
    "    # array\n",
    "    avg_arr = np.zeros(shape=listArr[0].shape)\n",
    "\n",
    "    # the amount will be if you want it to be by the tenth, hundreth or the thousandth\n",
    "    # for example .1 is tenth, .01 hundreth, .001 thousandth\n",
    "    multiplier = 1 / amount\n",
    "    # this is used to do the number of loops for adding each array expect the best array\n",
    "    loop_num = 0\n",
    "    # TODO need to fix this here using the values\n",
    "    if loss != None:\n",
    "        # loss must be a list\n",
    "        # need to find the lowest loss\n",
    "        comp = operator.lt\n",
    "        metric = loss\n",
    "        # setting to a high nunber for the loss to\n",
    "        # be able to find something that is lower than this\n",
    "        best_val = 1000 \n",
    "    else:\n",
    "        comp = operator.gt\n",
    "        metric = acc\n",
    "        best_val = 0\n",
    "    # doing the looping find the array that is the best\n",
    "    for i, val in enumerate(metric):\n",
    "        if comp(val, best_val):\n",
    "            best_val = val\n",
    "            best_arr = i\n",
    "    # adding the correct amount to each of the array\n",
    "    for i, arr in enumerate(listArr):\n",
    "        if i == best_arr:\n",
    "            # doing the best one into the avg_arr\n",
    "            divide_for_avg += multiplier\n",
    "            for _ in range(multiplier):\n",
    "                avg_arr += arr\n",
    "        else:\n",
    "            if loss != None:\n",
    "                # doing a loss\n",
    "                loop_num = round(((best_val/loss[i]) * multiplier))\n",
    "            else:\n",
    "                loop_num = round(((acc[i]/best_val) * multiplier))\n",
    "            divide_for_avg += loop_num\n",
    "            # doing the looping and adding the array value to a\n",
    "            for _ in range(loop_num):\n",
    "                avg_arr += arr\n",
    "    # will now divide by the number to get the average\n",
    "    avg_arr = avg_arr/divide_for_avg\n",
    "    return avg_arr\n",
    "    \n",
    "\n",
    "# This makes a list of the numpy array at the correct levl\n",
    "def  makeList(allWeights, level:int):\n",
    "    theList = []\n",
    "    for i in range(len(allWeights)):\n",
    "        the_list.append(allWeights[i][level])\n",
    "    return the_list\n",
    "\n",
    "\n",
    "def create_weight_avg(allWeights:list, loss=None, acc=None, amount=None):\n",
    "    \"\"\"\n",
    "    Function to create a average of the weights.\n",
    "\n",
    "    If we want to make the averages based on the loss we put a list of the losses \n",
    "    which will correspond to the weights.  If we want it based on the accuracy, \n",
    "    then we will put in a list of the accuracies for each of the weights.\n",
    "\n",
    "    Amount is used when doing loss or accuracy.  It is the amount of accuracy or loss precision.\n",
    "    can be .1, .01, .001 ect.\n",
    "\n",
    "    Returns:  will return the new list of the weights which can then be used to set the weights \n",
    "        of the model.\n",
    "    \"\"\"\n",
    "    # list of the numpy\n",
    "    numpyList = []\n",
    "    \n",
    "    # doing a loop\n",
    "    for i in range(len(allWeights[0])):\n",
    "        # making the val a numpy array\n",
    "        val = np.zeros(allWeights[0][i].shape)\n",
    "        # outher loop doing the number of the numpy arrays in each list  in the list\n",
    "        for j in range(len(allWeights)):\n",
    "            if loss != None or acc != None:\n",
    "                npList = makeList(allWeights, level=i)  \n",
    "                # calling the function to get the avg val\n",
    "                val = get_avg_with_metric(loss=loss, acc=acc, listArr=npList, amount= amount)\n",
    "            else:        \n",
    "                val += allWeights[j][i]\n",
    "        if loss != None or acc != None:        \n",
    "            # finding the average of the weights of one layer\n",
    "            val = val/len(allWeights)\n",
    "        # putting the val numpy array into the list\n",
    "        numpyList.append(val)\n",
    "    return numpyList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a custum callback\n",
    "class MyCallBack(keras.callbacks.Callback):\n",
    "    # function to save the loss and the acc and the model with \n",
    "    # early stopping\n",
    "    path = os.path.join(os.path.curdir, \"loss_metric\")\n",
    "\n",
    "    def check_if_not_empty(self):\n",
    "        with open(path, \"r\") as f:\n",
    "            f.seek(0, os.SEEK_END)\n",
    "            if my_file.tell():\n",
    "                my_file.seek(0)\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.check_if_not_empty():\n",
    "            with open(path, \"rw\") as f:\n",
    "                f.\n",
    "        \n",
    "\n",
    "        print(\n",
    "            \"The average loss for epoch {} is {:7.2f} \"\n",
    "            \"and mean absolute error is {:7.2f}.\".format(\n",
    "                epoch, logs[\"loss\"], logs[\"mean_absolute_error\"]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the data and then will reshape the data\n",
    "train, test = load_images()\n",
    "# reshaping the data\n",
    "end = len(train)\n",
    "train_images1, train_labels1 = reshape_data(train,start_index=-1000, end_index=len(train[0]))\n",
    "test_images1, test_labels1 = reshape_data(test, start_index=-1000, end_index=len(test[0]))\n",
    "\n",
    "train_images2 , train_labels2 = reshape_data(train, start_index=0, end_index=1000)\n",
    "test_images2, test_labels2 = reshape_data(test, start_index=0, end_index=1000)\n",
    "\n",
    "train_images3 , train_labels3 = reshape_data(train, start_index=3000, end_index=4000)\n",
    "test_images3, test_labels3 = reshape_data(test, start_index=3000, end_index=4000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((1000, 784), (1000, 784), (1000, 784), (1000, 784), (1000, 784))"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# checking the shapes of all the items\n",
    "train_images1.shape, train_images2.shape, train_images3.shape, test_images1.shape, test_images3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((1000, 784), (1000, 784), (1000, 784))"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "# looking at the test data\n",
    "test_images1.shape, test_images2.shape, test_images3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now building all the models\n",
    "model1 = create_model()\n",
    "model2 = create_model()\n",
    "model3 = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 1.5935 - sparse_categorical_accuracy: 0.5950 - val_loss: 1.0389 - val_sparse_categorical_accuracy: 0.7870\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.5846 - sparse_categorical_accuracy: 0.9050 - val_loss: 0.6443 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.3168 - sparse_categorical_accuracy: 0.9280 - val_loss: 0.5242 - val_sparse_categorical_accuracy: 0.8480\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.2007 - sparse_categorical_accuracy: 0.9530 - val_loss: 0.4667 - val_sparse_categorical_accuracy: 0.8660\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1551 - sparse_categorical_accuracy: 0.9700 - val_loss: 0.4566 - val_sparse_categorical_accuracy: 0.8580\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1123 - sparse_categorical_accuracy: 0.9800 - val_loss: 0.4377 - val_sparse_categorical_accuracy: 0.8690\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0877 - sparse_categorical_accuracy: 0.9850 - val_loss: 0.4378 - val_sparse_categorical_accuracy: 0.8680\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0746 - sparse_categorical_accuracy: 0.9910 - val_loss: 0.4295 - val_sparse_categorical_accuracy: 0.8680\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0660 - sparse_categorical_accuracy: 0.9910 - val_loss: 0.4218 - val_sparse_categorical_accuracy: 0.8760\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0549 - sparse_categorical_accuracy: 0.9940 - val_loss: 0.4268 - val_sparse_categorical_accuracy: 0.8740\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1cc75740460>"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# now doing the trainig of each of the models\n",
    "model1.fit(x=train_images1, y=train_labels1, batch_size=128, epochs=10,\n",
    "            validation_data=(test_images1, test_labels1))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.7361 - sparse_categorical_accuracy: 0.5120 - val_loss: 1.2145 - val_sparse_categorical_accuracy: 0.7140\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.7856 - sparse_categorical_accuracy: 0.8160 - val_loss: 0.7657 - val_sparse_categorical_accuracy: 0.7780\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.4889 - sparse_categorical_accuracy: 0.8760 - val_loss: 0.6034 - val_sparse_categorical_accuracy: 0.8220\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.3628 - sparse_categorical_accuracy: 0.9030 - val_loss: 0.5460 - val_sparse_categorical_accuracy: 0.8200\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2938 - sparse_categorical_accuracy: 0.9150 - val_loss: 0.4783 - val_sparse_categorical_accuracy: 0.8510\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.2423 - sparse_categorical_accuracy: 0.9420 - val_loss: 0.4609 - val_sparse_categorical_accuracy: 0.8510\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2028 - sparse_categorical_accuracy: 0.9550 - val_loss: 0.4448 - val_sparse_categorical_accuracy: 0.8620\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1771 - sparse_categorical_accuracy: 0.9570 - val_loss: 0.4381 - val_sparse_categorical_accuracy: 0.8640\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1483 - sparse_categorical_accuracy: 0.9720 - val_loss: 0.4245 - val_sparse_categorical_accuracy: 0.8620\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1276 - sparse_categorical_accuracy: 0.9780 - val_loss: 0.4126 - val_sparse_categorical_accuracy: 0.8730\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1cc06e6b730>"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "model2.fit(x=train_images2, y=train_labels2, batch_size=128, epochs=10,\n",
    "            validation_data=(test_images2, test_labels2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 1.7076 - sparse_categorical_accuracy: 0.5130 - val_loss: 1.1651 - val_sparse_categorical_accuracy: 0.7510\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.7620 - sparse_categorical_accuracy: 0.8480 - val_loss: 0.7359 - val_sparse_categorical_accuracy: 0.8000\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.4554 - sparse_categorical_accuracy: 0.8910 - val_loss: 0.5877 - val_sparse_categorical_accuracy: 0.8210\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.3183 - sparse_categorical_accuracy: 0.9250 - val_loss: 0.5261 - val_sparse_categorical_accuracy: 0.8420\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.2480 - sparse_categorical_accuracy: 0.9440 - val_loss: 0.4929 - val_sparse_categorical_accuracy: 0.8430\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1982 - sparse_categorical_accuracy: 0.9560 - val_loss: 0.4807 - val_sparse_categorical_accuracy: 0.8550\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1574 - sparse_categorical_accuracy: 0.9760 - val_loss: 0.4726 - val_sparse_categorical_accuracy: 0.8540\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1397 - sparse_categorical_accuracy: 0.9830 - val_loss: 0.4601 - val_sparse_categorical_accuracy: 0.8580\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1116 - sparse_categorical_accuracy: 0.9850 - val_loss: 0.4546 - val_sparse_categorical_accuracy: 0.8610\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0946 - sparse_categorical_accuracy: 0.9880 - val_loss: 0.4512 - val_sparse_categorical_accuracy: 0.8620\n"
     ]
    }
   ],
   "source": [
    "history = model3.fit(x=train_images3, y=train_labels3, batch_size=128, epochs=10, \n",
    "                        validation_data=(test_images3, test_labels3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'tensorflow.python.keras.callbacks.History'>\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1cc0a370e50>"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "print(type(history))\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'list'> 4\n"
     ]
    }
   ],
   "source": [
    "# using the create_weight_avg\n",
    "w1 = model.get_weights()\n",
    "w2 = model2.get_weights()\n",
    "print(type(w1), len(w1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the list \n",
    "the_list = [w1, w2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_weights = create_weight_avg(the_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(784, 512)"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "new_weights[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the model with the new weights\n",
    "model.set_weights(new_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5307 - sparse_categorical_accuracy: 0.8900\n"
     ]
    }
   ],
   "source": [
    "# trying to check with the val are with the evaluate\n",
    "theLoss, theAcc = model.evaluate(x=test_images, y=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_images, t_labels = reshape_data(test, len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((9998, 784), (9998,))"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "t_images.shape, t_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.6267 - sparse_categorical_accuracy: 0.8631\n"
     ]
    }
   ],
   "source": [
    "\n",
    "theLoss, theAcc = model.evaluate(x=t_images, y=t_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}