{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "tensorflow_env",
   "display_name": "tensorflow_env",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import os\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "# importing the operator module to do some comparisons\n",
    "import operator\n",
    "import datetime\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing the loading of the tensorboard\n",
    "#%load_ext tensorboard\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(useTensorboard=False):\n",
    "    \"\"\"\n",
    "    The function to create a model\n",
    "    \"\"\"\n",
    "    inputs = keras.layers.Input(shape=(784,))\n",
    "    t = keras.layers.Dense(512, activation=\"relu\", )(inputs)\n",
    "    t = keras.layers.Dropout(.2)(t)\n",
    "    out = keras.layers.Dense(10, )(t)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=out)\n",
    "\n",
    "   \n",
    "    # doing the compilation of the model\n",
    "    model.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the model\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'.\\\\..\\\\checkPoints/cp.ckpt'"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "# getting the path\n",
    "path = os.path.join(os.path.curdir, \"..\", \"checkPoints/cp.ckpt\")\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x255ce3cab20>"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "model.load_weights(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"functional_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         [(None, 784)]             0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 512)               401920    \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 512)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 10)                5130      \n=================================================================\nTotal params: 407,050\nTrainable params: 407,050\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# showing the summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The shape of what is returned with the weigths is ---- 0\nThe type is --- <class 'list'>\nThese are the weights for the input_1 layer --- []\nThe shape of what is returned with the weigths is ---- 2\nThe type is --- <class 'list'>\nThe type for each item in the list is: <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\nThese are the weights for the dense layer --- [<tf.Variable 'dense/kernel:0' shape=(784, 512) dtype=float32, numpy=\narray([[-0.05626954, -0.05369684, -0.06167276, ..., -0.001957  ,\n         0.00331007,  0.05483547],\n       [-0.049044  , -0.04101725,  0.04828054, ...,  0.0366065 ,\n         0.00066533,  0.05718373],\n       [ 0.00024945,  0.05019872, -0.0286467 , ...,  0.05689124,\n        -0.01853205,  0.05268286],\n       ...,\n       [-0.02224023, -0.06074744,  0.06550466, ..., -0.06264395,\n        -0.02794588, -0.01869341],\n       [ 0.03836417, -0.06636625, -0.0272741 , ...,  0.06481206,\n         0.06491137,  0.03532643],\n       [-0.02355461, -0.01153382,  0.01450439, ..., -0.02485488,\n        -0.00436073,  0.04444816]], dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(512,) dtype=float32, numpy=\narray([ 1.72597487e-04,  1.05121862e-02, -9.93606634e-03,  1.65048260e-02,\n        1.48517517e-02,  1.02724703e-02,  2.61683874e-02,  7.80888041e-03,\n       -6.67838706e-03, -2.63294880e-03,  7.54132587e-03,  1.07546821e-02,\n       -3.55655327e-04,  4.24108701e-03,  6.80069160e-03,  9.35562234e-03,\n       -1.48517676e-02, -1.26193035e-02,  2.14740597e-02,  1.90498773e-02,\n        1.75802875e-02,  1.00164497e-02,  2.33117156e-02,  5.82871586e-03,\n        1.24599226e-02,  2.49987701e-03, -4.81321290e-03, -3.32437549e-03,\n        2.52118185e-02,  1.74722690e-02,  5.97302278e-04,  5.02793957e-03,\n        1.51756210e-02,  8.94126110e-03,  1.02106249e-02,  1.77973378e-02,\n        1.91553961e-02, -4.73733991e-03,  6.50379900e-03, -1.48366541e-02,\n        9.10403393e-03,  5.40261390e-03,  1.62270386e-02,  1.17752440e-02,\n       -5.23779681e-03, -6.07746793e-03,  5.50066493e-03,  3.80394142e-03,\n        1.03695886e-02,  1.43970409e-02,  9.94092785e-03,  9.76029492e-04,\n       -7.81322550e-03, -2.01762130e-04,  3.46075930e-03,  1.20185986e-02,\n       -1.87506294e-03,  6.11349475e-03,  1.68360770e-02,  1.68225840e-02,\n       -2.46017892e-03, -1.42869249e-03,  1.85877215e-02,  1.65404603e-02,\n        2.38595940e-02,  1.01817586e-02,  2.15113480e-02,  1.31973531e-02,\n       -5.73000079e-03,  1.58557054e-02, -4.49281745e-03,  2.04001972e-03,\n        4.20698710e-03,  1.88715048e-02,  4.06323420e-03, -3.21356405e-04,\n        1.24803148e-02,  1.13657508e-02, -1.42590879e-02, -6.26258180e-03,\n        8.94737802e-03,  1.17486324e-02, -1.33981407e-02,  6.00477168e-03,\n        3.67667386e-03,  1.21088652e-02,  1.48060801e-03,  1.78699940e-02,\n        1.67217135e-04,  4.69582807e-03,  1.36005282e-02,  2.41139308e-02,\n       -1.46972360e-02,  1.36002703e-02,  8.92073568e-03,  2.23400593e-02,\n        5.55964792e-03,  1.95225626e-02,  1.62121840e-02,  3.65433225e-04,\n        2.03223117e-02,  9.67860874e-03, -6.90692523e-03,  1.18965907e-02,\n        1.19682113e-02, -4.15906496e-03,  8.81340820e-03, -1.83345713e-02,\n        1.10727884e-02,  1.21528478e-02, -1.50002241e-02,  1.79905035e-02,\n        1.85826402e-02, -3.98032740e-03, -4.49450081e-03,  1.26722511e-02,\n        1.90368434e-03,  3.97697091e-03, -1.27569510e-04,  7.51278829e-03,\n       -4.52209311e-03,  8.58710706e-03,  1.38698379e-02, -2.46764021e-03,\n        1.09259719e-02, -5.99331968e-03, -6.49669115e-03,  2.59255469e-02,\n        1.07646296e-02, -9.94778238e-04,  1.35442580e-03,  6.43221429e-03,\n        1.55213065e-02, -1.03907716e-02,  1.67679079e-02,  1.48895886e-02,\n       -1.18476804e-02,  1.07257171e-02,  3.55914212e-03,  1.11360867e-02,\n        2.93036830e-03, -3.82516207e-03, -7.80127384e-03, -2.39326619e-03,\n        2.46371757e-02, -1.27290459e-02,  4.07224707e-03,  2.12547034e-02,\n        1.63449980e-02, -1.96036268e-02,  1.01368725e-02,  5.91298752e-03,\n       -5.23868762e-03,  1.61788203e-02, -4.67349915e-03,  1.06823221e-02,\n        5.90169919e-04, -6.95585785e-03,  1.34356255e-02,  9.71553940e-03,\n        1.97174270e-02,  1.70956999e-02,  1.06709301e-02, -6.54631713e-03,\n       -7.34526431e-04,  1.74862556e-02,  1.42350337e-02, -6.98204851e-04,\n        1.40222372e-03,  9.99701302e-03, -5.94796031e-04, -3.38633684e-03,\n        6.83624577e-03,  2.06210911e-02,  6.89667277e-03, -9.13625769e-03,\n        2.23413692e-03,  1.46558462e-02, -1.52699575e-02,  1.26199508e-02,\n       -3.74557939e-03,  1.70344543e-02,  1.93049088e-02, -9.92825907e-03,\n        1.24239745e-02,  1.85128339e-02, -1.59667118e-03,  1.87675506e-02,\n        1.22579010e-02,  2.29209736e-02,  1.78051554e-02,  4.82282881e-03,\n        1.65875424e-02,  1.39386905e-02,  1.37819564e-02, -6.30658527e-04,\n       -9.30631813e-03,  1.08812954e-02,  1.87493283e-02,  1.71661433e-02,\n        1.36542949e-03,  3.62640317e-03,  8.66733212e-03,  2.22192612e-02,\n       -9.54966247e-03,  2.52975114e-02, -6.86078239e-03,  3.23635247e-03,\n        2.44395831e-03,  1.17151979e-02,  5.28266234e-03,  4.39155614e-03,\n        6.83129486e-03,  2.26570442e-02,  1.96919180e-02, -5.14318841e-03,\n       -1.46707119e-02,  1.50537351e-02, -7.96610583e-03, -5.84941404e-03,\n       -1.04225511e-02, -3.48156667e-03,  6.20870618e-03,  8.47083516e-04,\n       -3.22766340e-04,  2.02458296e-02,  8.79207999e-03, -1.08482623e-02,\n       -4.10871720e-03,  1.82971377e-02,  1.92662328e-02,  2.38690572e-03,\n       -3.01049603e-03, -7.71063007e-03,  7.57430261e-03,  5.64594427e-03,\n        4.10549762e-03,  1.92663120e-03, -3.68727138e-03, -1.50720710e-02,\n        4.26016469e-03,  8.37763958e-03,  1.87635794e-02,  1.72189120e-02,\n        1.94656663e-02,  7.15053687e-03,  8.85615405e-03, -1.42346381e-03,\n       -9.99983400e-03,  2.16865949e-02, -1.71931449e-03,  3.17133451e-03,\n        3.99380783e-03,  1.99397691e-02,  2.40188669e-02,  1.31260483e-02,\n        3.68883112e-03,  1.27733145e-02, -3.24683590e-03, -3.65647790e-03,\n        2.10799966e-02,  1.31309489e-02,  1.81905795e-02,  5.53172082e-04,\n        1.09740598e-02,  1.67485494e-02,  1.38644679e-02,  2.56677456e-02,\n       -5.92459692e-03, -9.01204161e-03,  1.50966719e-02,  1.90052949e-02,\n        5.04563795e-03,  1.95423011e-02, -2.22481205e-03,  2.00304314e-02,\n       -2.42300448e-03,  1.94869041e-02,  1.60062574e-02,  1.85983982e-02,\n        1.06929727e-02, -7.58236181e-03,  2.66808104e-02, -1.34435343e-02,\n        1.13502573e-02,  2.58019683e-03, -9.92083363e-03, -1.13562159e-02,\n        2.50601186e-03,  1.24915186e-02,  1.46082081e-02,  9.80041642e-03,\n        2.44212858e-02,  4.38334094e-03,  2.25721393e-03,  2.59431899e-02,\n        1.03705528e-03,  5.60802687e-03,  2.73208413e-02,  1.78735913e-03,\n       -7.89626210e-05,  5.90488361e-03,  9.59941931e-03,  7.09745847e-03,\n        5.42883063e-03,  1.11090923e-02, -1.65096018e-03,  9.30715818e-03,\n        1.25081968e-02,  5.28563047e-03,  1.38063738e-02,  1.55692799e-02,\n        1.94110665e-02,  1.20160030e-02,  1.19931754e-02, -9.86145250e-03,\n       -4.20161895e-03,  1.11806933e-02, -1.16814449e-02,  1.95057283e-03,\n       -1.19410164e-03, -5.52692974e-04,  1.86904371e-02,  7.18024885e-03,\n        2.42474792e-03,  5.96098136e-04,  2.52153887e-03,  2.20747739e-02,\n        2.95085032e-02,  1.31511874e-02,  2.25789733e-02, -1.11722788e-04,\n        1.16808983e-02,  1.92412268e-02,  9.21589043e-03,  3.73866945e-03,\n        8.38876050e-03,  2.29975265e-02,  1.25467060e-02, -8.58375989e-03,\n        1.19069275e-02,  1.16868911e-03, -3.79161863e-03,  2.32451450e-04,\n        1.03091856e-03,  1.29203321e-02, -4.12527053e-03,  8.03037488e-04,\n        6.71883160e-03,  3.42380581e-03,  7.29215425e-03, -5.46654360e-03,\n        5.38758375e-03, -4.01928835e-03,  1.97195783e-02, -1.42078269e-02,\n        1.46365268e-02,  1.39304148e-02, -7.20077660e-05, -3.99321085e-03,\n        1.80188045e-02,  1.18029630e-02, -5.73757617e-03, -1.67114806e-04,\n        2.41491292e-03, -6.95342082e-04,  2.10326128e-02,  7.40283588e-03,\n       -1.23865912e-02,  8.13089591e-03,  3.47751984e-03,  1.52790127e-02,\n        1.62633304e-02,  6.99118851e-03,  1.67725086e-02,  2.04010010e-02,\n        7.17896549e-03, -6.63141068e-03, -2.67516077e-03, -3.48725333e-03,\n       -1.08487699e-02, -1.05346544e-02, -1.14916568e-03, -4.90661850e-03,\n        2.67180521e-03,  2.25566644e-02,  7.95148686e-03,  8.24715383e-03,\n       -6.75756112e-03, -6.13528304e-03,  1.26109244e-02,  1.65690240e-02,\n       -9.37484019e-03,  1.85696762e-02,  3.04171462e-02,  8.15941207e-03,\n        1.68669783e-02,  1.80480856e-04,  1.92198344e-02, -1.38751874e-02,\n        2.82430113e-03,  7.58931227e-03, -5.92449505e-05,  1.37020685e-02,\n        1.05779839e-03,  1.03384741e-02,  1.86582841e-02,  1.13291712e-02,\n       -2.57410631e-02,  6.47610752e-03,  2.68482175e-02,  2.21051555e-03,\n        3.20901396e-03,  7.53921317e-03, -7.31907692e-03,  1.43468492e-02,\n        2.40712725e-02,  5.22164768e-03,  8.10789689e-03,  1.42931417e-02,\n       -1.13822753e-02,  4.85085184e-03,  3.26441508e-03,  5.15708118e-04,\n        1.23437187e-02,  2.07592417e-02,  2.08405964e-03,  2.08609216e-02,\n       -7.50211300e-03,  2.43883003e-02,  1.71769746e-02,  3.45202279e-04,\n        7.74060842e-03,  1.75596327e-02,  1.49875330e-02,  1.00264782e-02,\n        2.27080137e-02, -4.31349734e-03, -8.22057715e-04, -5.30624529e-03,\n        1.44437999e-02,  1.01032453e-02,  3.02405711e-02,  1.30343810e-02,\n       -9.96138249e-03, -1.17465463e-02,  6.86999783e-03,  1.69459078e-02,\n        8.81494116e-03,  1.89694893e-02, -8.15362297e-03,  2.04268526e-02,\n        2.05217451e-02,  2.41035428e-02,  1.63802225e-02,  2.44297888e-02,\n        6.81259716e-03, -4.27511724e-04, -8.88006855e-03,  1.69149449e-03,\n       -1.00014545e-02, -2.10044812e-03,  2.84275389e-03,  5.16199833e-03,\n       -2.96031102e-03, -9.93147027e-03,  9.39159933e-03, -7.03254715e-04,\n       -5.56486985e-03,  1.94572415e-02,  5.54063357e-04,  1.27571467e-02,\n        6.32402534e-03,  1.54216932e-02,  2.88840686e-03,  4.95937327e-03,\n        1.87297799e-02, -1.37654459e-02,  3.01342495e-02,  1.98066961e-02,\n        4.67774924e-03,  1.97523683e-02,  3.84773174e-03,  1.51203284e-02,\n        1.42727140e-02,  2.30135377e-02, -1.43146319e-02,  1.01353768e-02,\n       -7.84549024e-03,  1.54151777e-02, -2.93102057e-04,  3.77435004e-03,\n        2.01666355e-02,  8.91412981e-03,  1.88908970e-03,  1.84706245e-02,\n       -1.27008101e-02,  1.77656468e-02,  4.96036559e-03,  6.27404312e-03,\n        2.41043326e-02, -9.00016259e-03,  2.04670690e-02,  4.75749187e-03,\n        1.76593731e-03,  1.06164468e-02,  1.41866617e-02, -2.15561278e-02,\n        4.11901297e-03, -1.51740536e-02,  9.53926146e-03,  2.51788497e-02],\n      dtype=float32)>]\nThe shape of what is returned with the weigths is ---- 0\nThe type is --- <class 'list'>\nThese are the weights for the dropout layer --- []\nThe shape of what is returned with the weigths is ---- 2\nThe type is --- <class 'list'>\nThe type for each item in the list is: <class 'tensorflow.python.ops.resource_variable_ops.ResourceVariable'>\nThese are the weights for the dense_1 layer --- [<tf.Variable 'dense_1/kernel:0' shape=(512, 10) dtype=float32, numpy=\narray([[ 0.03622122, -0.03641613,  0.12541647, ..., -0.03731978,\n        -0.03972755, -0.00030406],\n       [-0.09065956, -0.04421225, -0.00835247, ..., -0.06340674,\n        -0.14152284, -0.06728777],\n       [ 0.02464028, -0.04258672,  0.04578171, ..., -0.04067839,\n         0.03545604, -0.0039114 ],\n       ...,\n       [ 0.04221679,  0.08953761,  0.11619351, ...,  0.02745421,\n        -0.02400982, -0.07182321],\n       [-0.09616829, -0.06718431,  0.09197892, ..., -0.07621801,\n        -0.06656138, -0.13127476],\n       [ 0.00644363,  0.08017394,  0.00601754, ...,  0.12485438,\n        -0.04525638, -0.0403614 ]], dtype=float32)>, <tf.Variable 'dense_1/bias:0' shape=(10,) dtype=float32, numpy=\narray([-0.02103808,  0.00912677, -0.00462178, -0.01466964,  0.01683323,\n        0.01430766, -0.00607732,  0.01283323, -0.01082663, -0.00280234],\n      dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "# printing the weights of the layers\n",
    "counter = 0\n",
    "for layer in model.layers:\n",
    "    print(f\"The shape of what is returned with the weigths is ---- {len(layer.weights)}\")\n",
    "    print(f\"The type is --- {type(layer.weights)}\")\n",
    "    if len(layer.weights) > 0:\n",
    "        print(f\"The type for each item in the list is: {type(layer.weights[0])}\")\n",
    "    print(f\"These are the weights for the {layer.name} layer --- {layer.weights}\" )\n",
    "    counter += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the funtion that will create the bounds from where the  first indices\n",
    "# within a data window can be.  Then with a random a value will be chosen \n",
    "# from the possible data indices\n",
    "def begin_all_in_window(dataLength, all_in_per_data_window:int, data_window_size:int, start_index_of_data_window:int):\n",
    "    # The window end is not included in the window\n",
    "    windowEnd = start_index_of_data_window + data_window_size\n",
    "    # This is to make sure that it doesn't overstep the bounds per window\n",
    "    if windowEnd > dataLength:\n",
    "        # need to alter the amount of all_in_that can be used\n",
    "        windowEnd = dataLength \n",
    "        if all_in_per_data_window > (windowEnd - start_index_of_data_window ):\n",
    "            # need to change the size of the all_in_per_data_window\n",
    "            all_in_per_data_window = windowEnd - start_index_of_data_window \n",
    "\n",
    "    end_bound = windowEnd - all_in_per_data_window\n",
    "    choice = random.randint(start_index_of_data_window, end_bound)\n",
    "    return choice\n",
    "\n",
    "\n",
    "# This is the function that will get the beginning index of the next data window\n",
    "# if there is no more data windows will return false\n",
    "def get_next_data_window_index(dataSize:int, current_begin_window_index:int, data_window_size:int):\n",
    "    new_index = current_begin_window_index + data_window_size\n",
    "    if new_index >= dataSize or new_index + data_window_size >= dataSize:\n",
    "        return False\n",
    "    return new_index\n",
    "    \n",
    "\n",
    "\n",
    "# This is the function that will return the indices of the data\n",
    "# that is in all of the chunks of data.\n",
    "def get_in_all_chunks_indices(data, all_in_size:int, num_chunks_estimate:int, chunk_size:int):\n",
    "    indices_list = []\n",
    "    # checking to see if the data is a tuple\n",
    "    if isinstance(data, tuple):\n",
    "        # Will only look at one but the indices can be used\n",
    "        # for both data and the data_lables\n",
    "        data = data[0]\n",
    "    # Will go through the data by quarters\n",
    "    # from each quarter will grab 2 1/8th of the all_in size\n",
    "    data_window_size = int(len(data) /8)\n",
    "    # getting size of 1/8th of the all_in_size\n",
    "    all_in_per_data_window = int(all_in_size/8)\n",
    "    start_index_of_data_window = 0\n",
    "    # doing the loop that will get the indices\n",
    "    while True:\n",
    "        begin_all_in = begin_all_in_window(len(data), all_in_per_data_window, data_window_size, \n",
    "                                            start_index_of_data_window)\n",
    "        end = begin_all_in + all_in_per_data_window\n",
    "        # indices_list will contain a tuple of the begin and the end and the range between the two\n",
    "        # indices_list.append((begin_all_in, end, list(range(begin_all_in, end + 1))))\n",
    "        indices_to_add = list(range(begin_all_in, end + 1))\n",
    "        indices_list += indices_to_add\n",
    "\n",
    "        # moving to the next data window\n",
    "        start_index_of_data_window = get_next_data_window_index(len(data), \n",
    "                                                start_index_of_data_window, data_window_size)\n",
    "        if not start_index_of_data_window:\n",
    "            # breaking out if it is false\n",
    "            break\n",
    "    #TODO will make a set and a list that is returned\n",
    "    return indices_list\n",
    "\n",
    "\n",
    "# getting the data_chunk size\n",
    "def get_data_chunk_size(data_size:int, chunk_size:float, in_all:float):\n",
    "    chunked_size = int(data_size * chunk_size)\n",
    "    in_all =int(chunked_size * in_all)\n",
    "    original_data_per_chunk = chunked_size - in_all\n",
    "    # finding the number of chunck estimated to make\n",
    "    num_chunks_estimate = int((data_size - in_all)/ original_data_per_chunk)\n",
    "    return original_data_per_chunk, chunked_size, in_all, num_chunks_estimate\n",
    "\n",
    "# making the indices list\n",
    "def makeIndexList(chunkStart:int, windowSize:int,                                         all_in_indices_list:list, original_data_per_chunk:int):\n",
    "    indexList = []\n",
    "    created_window_size = 0\n",
    "    if \n",
    "    # adding the first index\n",
    "    indices_list.append(chunkStart)\n",
    "    created_window_size += 1\n",
    "    i_val = 0\n",
    "\n",
    "    # check if the \n",
    "    for i_val, theTuple in enumerate(all_in_indices_list):\n",
    "        # going through the list of the indexes\n",
    "        begin, end, index_vals = theTuple\n",
    "        if chunkStart >= begin and chunkStart <= end and created_window_size < windowSize:\n",
    "            # if in here fill with the in_all indexes\n",
    "\n",
    "\n",
    "# This is the function that will make the data_chunks\n",
    "def make_data_chunks(data_length:int, all_in_indices_list:list, orginal_data_size:int,\n",
    "                        chunked_window_size:int):\n",
    "    original_portion_window_size = 0\n",
    "    window_index = 0\n",
    "    chunk_size = 0\n",
    "    list_of_chunk_indexes = []\n",
    "    index_pos_choice = 0\n",
    "    # index list\n",
    "    indexList = []\n",
    "    chunk_indexes = []\n",
    "    # if isinstance(data, np.ndarray):\n",
    "    while     \n",
    "    \n",
    "    for i in range(len(indices_list)):\n",
    "        begin, end , index_val = indices_list[i]\n",
    "        # if the index is less than begin\n",
    "        # then will add upto the begin\n",
    "        # and then contiue after the end\n",
    "        if index_pos_choice == begin:\n",
    "            # build some of the chunk\n",
    "            chunk_indexes += index_val\n",
    "            index_pos_choice = end + 1\n",
    "\n",
    "        # now building the index list from the \n",
    "        # data not in the in_all indexes\n",
    "        if not orginalFilled:\n",
    "            for j in range(index_pos_choice, data_length):\n",
    "                if original_portion_window_size >= orginal_data_size:\n",
    "                    originalFilled = True\n",
    "                    window_index = index_pos_choice + 1  # TODO making sure this works\n",
    "                    break\n",
    "                if index_pos_choice != begin:\n",
    "                    # checking to see if we need to check the next batch of in_all indices\n",
    "                    if i < len(indices_list) -1:\n",
    "                        if index_pos_choice == indices_list[i+1][0]:\n",
    "                            orginalFilled = False\n",
    "                            break # breaking out of this loop\n",
    "                    # adding one index at a time\n",
    "                    chunk_indexes += [j]\n",
    "                    index_pos_choice += 1\n",
    "                    original_portion_window_size += 1\n",
    "    # adding this chunk index to the list of chunk indexes\n",
    "    list_of_chunk_indexes.append(chunk_indexes)\n",
    "\n",
    "                    \n",
    "\n",
    "    indexList, window_index = makeIndexList(chunkStart= window_index, windowSize=chunked_window_size,                                         all_in_indices_list=all_in_indices_list,                       original_data_per_chunk=orginal_data_size)\n",
    "    # need to build one of the chunks\n",
    "\n",
    "            \n",
    "\n",
    "# This is the function that will be used to get the data but have it so that there is \n",
    "# some of the data that is mixed in all of the data\n",
    "def chunk_shuffle(data, data_chunk_size=None, in_all=None ):\n",
    "    \"\"\"\n",
    "    This is the function that will get the data as chunks and having some \n",
    "    of the data found in each of the chunks.\n",
    "\n",
    "    data:   The data is the data passed into the function. Not a tuple\n",
    "\n",
    "    data_chunk_size:    This is the size of the data chunk that the function will try to return\n",
    "                        It is not guaranteed to get the exact amount of chunk size depending on the size \n",
    "                        of the data that is passed in the function. Data_chunk_size is a percentage or \n",
    "                        float that will be passed in.  For example if .8 would mean that each chunk_size                            will be 80% of the total data.\n",
    "\n",
    "    in_all:             This is the parameter that if passed in will have some of the data that is found in                         all of data chunks.  A float is expected as the variable. This float is as                                  percentage .8 means that each of the chunks will have 80% of the data found in each                         of the data_chunks.\n",
    "                        If not passed in then there will be no amount overlapping between data_chunks\n",
    "\n",
    "    Returns:            Will return a list of data_chunks\n",
    "    \"\"\"\n",
    "    if data_chunk_size == None:\n",
    "        raise Exception(\"You need to pass in a float value for the data_chunk size\")\n",
    "\n",
    "    data_length = None\n",
    "\n",
    "    if isinstance(data, tuple):\n",
    "        # will need to pass in to the get_data_chunk_size not a tuple\n",
    "        data_length = data[0]\n",
    "     \n",
    "    \n",
    "    # getting the sizes used in the making of the chunks\n",
    "    original_data_per_window_size, chunked_window_size, in_all_size, num_chunks_estimate = get_data_chunk_size(data_length,                                                                                   data_chunk_size,  in_all)\n",
    "    # getting the random data that is spread through all the data chunks\n",
    "    # will return a list of tuples, where each tuple has the start and the end\n",
    "    # indices for some of the data that is in all the chunks\n",
    "    # This function will check if the data is a tuple, if it is then all the data uses\n",
    "    # the same indices\n",
    "    in_all_indices_list = get_in_all_chunks_indices(data, in_all_size, num_chunks_estimate)\n",
    "\n",
    "    # making the data chunks\n",
    "    # need to make the original_data_size\n",
    "    # chunkStart:int, windowSize:int, all_in_indices_list:list,                             original_data_per_chunk:int\n",
    "    make_data_chunks(data_lenth, in_all_indices_list, orginal_data_size, chunked_window_size)\n",
    "\n",
    "\n",
    "# getting the data for another block  that is different from the one that the modle above is trained with.\n",
    "def load_images():\n",
    "    training, testing = keras.datasets.mnist.load_data()\n",
    "    # Each of these are tuples that contain ndarrays\n",
    "\n",
    "    return training, testing\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def reshape_data(data, start_index:int, end_index:int ):\n",
    "    images = None\n",
    "    labels = None\n",
    "    if isinstance(data, tuple):\n",
    "        images, labels = data\n",
    "    \n",
    "    # made the slice to pull from the opposite end\n",
    "    images = images[start_index: end_index].reshape(-1, (28 * 28))\n",
    "    labels = labels[start_index: end_index]\n",
    "    \n",
    "    images = images/255.0\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((1000, 784), (1000,))"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# now creating a model\n",
    "# looking at the shape of the train \n",
    "train_images.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 1.5915 - sparse_categorical_accuracy: 0.5840 - val_loss: 0.9791 - val_sparse_categorical_accuracy: 0.8110\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.5774 - sparse_categorical_accuracy: 0.8940 - val_loss: 0.6365 - val_sparse_categorical_accuracy: 0.8230\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.3004 - sparse_categorical_accuracy: 0.9370 - val_loss: 0.5017 - val_sparse_categorical_accuracy: 0.8500\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.2014 - sparse_categorical_accuracy: 0.9550 - val_loss: 0.4593 - val_sparse_categorical_accuracy: 0.8700\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1432 - sparse_categorical_accuracy: 0.9710 - val_loss: 0.4497 - val_sparse_categorical_accuracy: 0.8640\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1147 - sparse_categorical_accuracy: 0.9780 - val_loss: 0.4310 - val_sparse_categorical_accuracy: 0.8740\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0952 - sparse_categorical_accuracy: 0.9840 - val_loss: 0.4405 - val_sparse_categorical_accuracy: 0.8710\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0772 - sparse_categorical_accuracy: 0.9910 - val_loss: 0.4264 - val_sparse_categorical_accuracy: 0.8850\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0632 - sparse_categorical_accuracy: 0.9920 - val_loss: 0.4189 - val_sparse_categorical_accuracy: 0.8840\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0535 - sparse_categorical_accuracy: 0.9940 - val_loss: 0.4269 - val_sparse_categorical_accuracy: 0.8770\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x255cdd512e0>"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "model2.fit(x=train_images, y=train_labels, batch_size=128, epochs=10, \n",
    "            validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(list, tensorflow.python.ops.resource_variable_ops.ResourceVariable)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# getting the weights for this model\n",
    "the_weights = model2.weights\n",
    "type(the_weights), type(the_weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Variable 'dense/kernel:0' shape=(784, 512) dtype=float32, numpy=\n",
       "array([[ 2.0674616e-02,  3.2345042e-02, -7.7739358e-05, ...,\n",
       "        -6.2627956e-02, -3.8801931e-02, -1.3471089e-02],\n",
       "       [ 2.7597107e-02,  5.6585222e-02,  1.0034814e-03, ...,\n",
       "         3.5107583e-03, -3.3186216e-02, -1.6239803e-02],\n",
       "       [ 2.3986742e-02,  2.8475918e-02,  1.3272785e-02, ...,\n",
       "        -1.3933085e-02,  1.9999221e-03,  1.2072645e-02],\n",
       "       ...,\n",
       "       [ 8.9056566e-03,  3.4107566e-02, -5.5254426e-02, ...,\n",
       "        -5.1518083e-03,  1.5794642e-02,  3.3432893e-02],\n",
       "       [-2.1421652e-02,  2.4194144e-02,  2.5040790e-02, ...,\n",
       "        -2.1030013e-02,  6.0876384e-02, -6.0057208e-02],\n",
       "       [-3.4105852e-03,  2.0341501e-03,  6.6730469e-02, ...,\n",
       "        -4.5968406e-02,  2.5523305e-02,  5.9543997e-02]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "the_weights[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(numpy.ndarray, (784, 512))"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# getting the weights of the model\n",
    "w = the_weights[0].numpy()\n",
    "type(w), w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(numpy.ndarray, (784, 512))"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "# getting the weights of the first model above\n",
    "first_w = model.weights[0].numpy()\n",
    "type(first_w), w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "# getting the average of the two weights\n",
    "avg_wt = (w + first_w)/2\n",
    "type(avg_wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'list'> <class 'numpy.ndarray'> 4\n-first is (784, 512), second (512,), third (512, 10), 4th is (10,)\n"
     ]
    }
   ],
   "source": [
    "# getting the item of the tensorflow variables\n",
    "the_var_weights = model2.get_weights()\n",
    "print(type(the_var_weights),  type(the_var_weights[0]), len(the_var_weights))\n",
    "print(f\"-first is {the_var_weights[0].shape}, second {the_var_weights[1].shape}, third {the_var_weights[2].shape}, 4th is {the_var_weights[3].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the function that will give the loss or the accuracy in a list\n",
    "def get_loss_or_acc(historyList:list, loss=None, acc=None):\n",
    "    \"\"\"\n",
    "    param:  Loss should be  the type of loss (string) that is found in the dictionary of history\n",
    "            Acc if not None should be a string of the name that person wants to get from the history\n",
    "            can only have either the loss or the acc passed in.\n",
    "\n",
    "    Returns:    Will return a list of the loss or the acc in the order that the histories are passed in.\n",
    "                Returns the last value in the history list.\n",
    "    \"\"\"\n",
    "    if loss == None and acc == None:\n",
    "        raise Exception (\"Need to have at either loss or acc not be None\")\n",
    "    value_list = []\n",
    "    item  = loss\n",
    "    if acc:\n",
    "        item = acc\n",
    "    \n",
    "    for h in historyList:\n",
    "        value_list.append(h.history[item][-1])\n",
    "    return value_list\n",
    "\n",
    "\n",
    "# This function will find the average using the loss or the acc \n",
    "# If using the loss the best one is the one with the least loss \n",
    "# If using the acc then the best one is the one with the highest acc\n",
    "def get_avg_with_metric(listArr:list, amount:float, loss=None, acc=None ):\n",
    "    best_val = None\n",
    "    best_arr = 0\n",
    "    metric = None\n",
    "\n",
    "    # this is the number to divide by to get the average\n",
    "    divide_for_avg = 0\n",
    "    # array that will hold all the values and will hold the end result of the avg \n",
    "    # array\n",
    "    avg_arr = np.zeros(shape=listArr[0].shape)\n",
    "\n",
    "    # the amount will be if you want it to be by the tenth, hundreth or the thousandth\n",
    "    # for example .1 is tenth, .01 hundreth, .001 thousandth\n",
    "    multiplier =int(1 / amount)\n",
    "    # this is used to do the number of loops for adding each array expect the best array\n",
    "    loop_num = 0\n",
    "    # TODO need to fix this here using the values\n",
    "    if loss != None:\n",
    "        # loss must be a list\n",
    "        # need to find the lowest loss\n",
    "        comp = operator.lt\n",
    "        metric = loss\n",
    "        # setting to a high nunber for the loss to\n",
    "        # be able to find something that is lower than this\n",
    "        best_val = 1000 \n",
    "    else:\n",
    "        comp = operator.gt\n",
    "        metric = acc\n",
    "        best_val = 0\n",
    "    # doing the looping find the array that is the best\n",
    "    for i, val in enumerate(metric):\n",
    "        if comp(val, best_val):\n",
    "            best_val = val\n",
    "            best_arr = i\n",
    "    # adding the correct amount to each of the array\n",
    "    for i, arr in enumerate(listArr):\n",
    "        if i == best_arr:\n",
    "            # doing the best one into the avg_arr\n",
    "            divide_for_avg += multiplier\n",
    "            for _ in range(multiplier):\n",
    "                avg_arr += arr\n",
    "        else:\n",
    "            if loss != None:\n",
    "                # doing a loss\n",
    "                loop_num = round(((best_val/loss[i]) * multiplier))\n",
    "            else:\n",
    "                loop_num = round(((acc[i]/best_val) * multiplier))\n",
    "            divide_for_avg += loop_num\n",
    "            # doing the looping and adding the array value to a\n",
    "            for _ in range(loop_num):\n",
    "                avg_arr += arr\n",
    "    # will now divide by the number to get the average\n",
    "    avg_arr = avg_arr/divide_for_avg\n",
    "    return avg_arr\n",
    "    \n",
    "\n",
    "# This makes a list of the numpy array at the correct levl\n",
    "def  makeList(allWeights, level:int):\n",
    "    theList = []\n",
    "    for i in range(len(allWeights)):\n",
    "        theList.append(allWeights[i][level])\n",
    "    return theList\n",
    "\n",
    "\n",
    "def create_weight_avg(allWeights:list, loss=None, acc=None, amount=None):\n",
    "    \"\"\"\n",
    "    Function to create a average of the weights.\n",
    "\n",
    "    If we want to make the averages based on the loss we put a list of the losses \n",
    "    which will correspond to the weights.  If we want it based on the accuracy, \n",
    "    then we will put in a list of the accuracies for each of the weights.\n",
    "\n",
    "    Amount is used when doing loss or accuracy.  It is the amount of accuracy or loss precision.\n",
    "    can be .1, .01, .001 ect.\n",
    "\n",
    "    Returns:  will return the new list of the weights which can then be used to set the weights \n",
    "        of the model.\n",
    "    \"\"\"\n",
    "    # list of the numpy\n",
    "    numpyList = []\n",
    "    \n",
    "    # doing a loop\n",
    "    for i in range(len(allWeights[0])):\n",
    "        # making the val a numpy array\n",
    "        val = np.zeros(allWeights[0][i].shape)\n",
    "        # outher loop doing the number of the numpy arrays in each list  in the list\n",
    "        for j in range(len(allWeights)):\n",
    "            if loss != None or acc != None:\n",
    "                npList = makeList(allWeights, level=i)  \n",
    "                # calling the function to get the avg val\n",
    "                val = get_avg_with_metric(loss=loss, acc=acc, listArr=npList, amount= amount)\n",
    "            else:        \n",
    "                val += allWeights[j][i]\n",
    "        if loss != None or acc != None:        \n",
    "            # finding the average of the weights of one layer\n",
    "            val = val/len(allWeights)\n",
    "        # putting the val numpy array into the list\n",
    "        numpyList.append(val)\n",
    "    return numpyList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the data and then will reshape the data\n",
    "train, test = load_images()\n",
    "# reshaping the data\n",
    "end = len(train)\n",
    "train_images1, train_labels1 = reshape_data(train,start_index=-1000, end_index=len(train[0]))\n",
    "test_images1, test_labels1 = reshape_data(test, start_index=-1000, end_index=len(test[0]))\n",
    "\n",
    "train_images2 , train_labels2 = reshape_data(train, start_index=0, end_index=1000)\n",
    "test_images2, test_labels2 = reshape_data(test, start_index=0, end_index=1000)\n",
    "\n",
    "train_images3 , train_labels3 = reshape_data(train, start_index=3000, end_index=4000)\n",
    "test_images3, test_labels3 = reshape_data(test, start_index=3000, end_index=4000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((1000, 784), (1000, 784), (1000, 784), (1000, 784), (1000, 784))"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# checking the shapes of all the items\n",
    "train_images1.shape, train_images2.shape, train_images3.shape, test_images1.shape, test_images3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((1000, 784), (1000, 784), (1000, 784))"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# looking at the test data\n",
    "test_images1.shape, test_images2.shape, test_images3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now building all the models\n",
    "model1 = create_model()\n",
    "model2 = create_model()\n",
    "model3 = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 1.5954 - sparse_categorical_accuracy: 0.5760 - val_loss: 1.0072 - val_sparse_categorical_accuracy: 0.7970\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.5810 - sparse_categorical_accuracy: 0.9000 - val_loss: 0.6211 - val_sparse_categorical_accuracy: 0.8330\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.3044 - sparse_categorical_accuracy: 0.9390 - val_loss: 0.5042 - val_sparse_categorical_accuracy: 0.8470\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1996 - sparse_categorical_accuracy: 0.9510 - val_loss: 0.4536 - val_sparse_categorical_accuracy: 0.8620\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1534 - sparse_categorical_accuracy: 0.9660 - val_loss: 0.4424 - val_sparse_categorical_accuracy: 0.8670\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1181 - sparse_categorical_accuracy: 0.9770 - val_loss: 0.4261 - val_sparse_categorical_accuracy: 0.8700\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0936 - sparse_categorical_accuracy: 0.9820 - val_loss: 0.4255 - val_sparse_categorical_accuracy: 0.8800\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0770 - sparse_categorical_accuracy: 0.9880 - val_loss: 0.4364 - val_sparse_categorical_accuracy: 0.8720\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0700 - sparse_categorical_accuracy: 0.9880 - val_loss: 0.4173 - val_sparse_categorical_accuracy: 0.8760\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0566 - sparse_categorical_accuracy: 0.9920 - val_loss: 0.4180 - val_sparse_categorical_accuracy: 0.8820\n"
     ]
    }
   ],
   "source": [
    "# now doing the trainig of each of the models\n",
    "h1 = model1.fit(x=train_images1, y=train_labels1, batch_size=128, epochs=10,\n",
    "            validation_data=(test_images1, test_labels1))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 1.7102 - sparse_categorical_accuracy: 0.5110 - val_loss: 1.1736 - val_sparse_categorical_accuracy: 0.7500\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.7866 - sparse_categorical_accuracy: 0.8160 - val_loss: 0.7552 - val_sparse_categorical_accuracy: 0.7900\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.4880 - sparse_categorical_accuracy: 0.8740 - val_loss: 0.6209 - val_sparse_categorical_accuracy: 0.8170\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.3728 - sparse_categorical_accuracy: 0.8930 - val_loss: 0.5403 - val_sparse_categorical_accuracy: 0.8340\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.3031 - sparse_categorical_accuracy: 0.9150 - val_loss: 0.4990 - val_sparse_categorical_accuracy: 0.8380\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.2511 - sparse_categorical_accuracy: 0.9350 - val_loss: 0.4642 - val_sparse_categorical_accuracy: 0.8550\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.2064 - sparse_categorical_accuracy: 0.9560 - val_loss: 0.4504 - val_sparse_categorical_accuracy: 0.8610\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1810 - sparse_categorical_accuracy: 0.9580 - val_loss: 0.4348 - val_sparse_categorical_accuracy: 0.8570\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1592 - sparse_categorical_accuracy: 0.9650 - val_loss: 0.4331 - val_sparse_categorical_accuracy: 0.8620\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1365 - sparse_categorical_accuracy: 0.9790 - val_loss: 0.4296 - val_sparse_categorical_accuracy: 0.8620\n"
     ]
    }
   ],
   "source": [
    "h2 = model2.fit(x=train_images2, y=train_labels2, batch_size=128, epochs=10,\n",
    "            validation_data=(test_images2, test_labels2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 1.7538 - sparse_categorical_accuracy: 0.4850 - val_loss: 1.1854 - val_sparse_categorical_accuracy: 0.7680\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.7992 - sparse_categorical_accuracy: 0.8430 - val_loss: 0.7352 - val_sparse_categorical_accuracy: 0.7870\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.4765 - sparse_categorical_accuracy: 0.8880 - val_loss: 0.5941 - val_sparse_categorical_accuracy: 0.8240\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.3406 - sparse_categorical_accuracy: 0.9130 - val_loss: 0.5281 - val_sparse_categorical_accuracy: 0.8340\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.2558 - sparse_categorical_accuracy: 0.9410 - val_loss: 0.4922 - val_sparse_categorical_accuracy: 0.8450\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.2053 - sparse_categorical_accuracy: 0.9610 - val_loss: 0.4767 - val_sparse_categorical_accuracy: 0.8530\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1754 - sparse_categorical_accuracy: 0.9670 - val_loss: 0.4657 - val_sparse_categorical_accuracy: 0.8500\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1447 - sparse_categorical_accuracy: 0.9700 - val_loss: 0.4551 - val_sparse_categorical_accuracy: 0.8570\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1191 - sparse_categorical_accuracy: 0.9830 - val_loss: 0.4462 - val_sparse_categorical_accuracy: 0.8670\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1019 - sparse_categorical_accuracy: 0.9870 - val_loss: 0.4459 - val_sparse_categorical_accuracy: 0.8610\n"
     ]
    }
   ],
   "source": [
    "h3 = model3.fit(x=train_images3, y=train_labels3, batch_size=128, epochs=10, \n",
    "                        validation_data=(test_images3, test_labels3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "histList = get_loss_or_acc([h1, h2, h3], loss=\"val_loss\")\n",
    "histList2 = get_loss_or_acc([h1,h2,h3], acc=\"val_sparse_categorical_accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.41800546646118164, 0.4295928478240967, 0.4458629786968231]"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "histList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.8820000290870667, 0.8619999885559082, 0.8610000014305115]"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "histList2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "# Trying to get the new weights for the models\n",
    "w1 = model1.get_weights()\n",
    "w2 = model2.get_weights()\n",
    "w3 = model3.get_weights()\n",
    "type(w1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the new weights\n",
    "new_weights_from_loss = create_weight_avg(allWeights=[w1, w2, w3], amount=.01, loss=histList)\n",
    "new_weights_from_acc = create_weight_avg(allWeights=[w1, w2, w3], acc=histList2, amount=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(list, 4, (784, 512))"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "type(new_weights_from_loss) ,len(new_weights_from_loss), new_weights_from_loss[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(list, 4, (784, 512))"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "# comparing to the first weights\n",
    "type(w1), len(w1), w1[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a totally new model\n",
    "loss_model = create_model()\n",
    "loss_model.set_weights(new_weights_from_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_model = create_model()\n",
    "acc_model.set_weights(new_weights_from_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running through all the data\n",
    "test_images, test_labels = reshape_data(test, start_index=0, end_index=len(test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.5259 - sparse_categorical_accuracy: 0.8435\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.5259408950805664, 0.843500018119812]"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "model1.evaluate(x=test_images, y=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3877 - sparse_categorical_accuracy: 0.8809\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.38773971796035767, 0.8809000253677368]"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "model2.evaluate(x=test_images, y=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.3809 - sparse_categorical_accuracy: 0.8817\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.38086366653442383, 0.8816999793052673]"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "model3.evaluate(x=test_images, y=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 2.0591 - sparse_categorical_accuracy: 0.8555\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[2.0591301918029785, 0.8554999828338623]"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "# doing the evaluation of the models \n",
    "loss_model.evaluate(x=test_images, y=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 2.0589 - sparse_categorical_accuracy: 0.8559\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[2.058875322341919, 0.85589998960495]"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "# doing the evaluation of the model where we looked at the best accuracy\n",
    "acc_model.evaluate(x=test_images, y=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking to see what the accuracy will be with the model trained on all the \n",
    "# data.\n",
    "all_data_model = create_model()\n",
    "# getting the data for the model\n",
    "train, test = load_images()\n",
    "# getting the data in the correct format\n",
    "train_images, train_labels = reshape_data(train, start_index=0, end_index=len(train[0]))\n",
    "test_images , test_labels = reshape_data(test, start_index=0, end_index=len(test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a callback for the tensorboard\n",
    "the_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# creating the tensorboard callback\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=the_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "  2/469 [..............................] - ETA: 1:31 - loss: 0.0021 - sparse_categorical_accuracy: 1.0000WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0050s vs `on_train_batch_end` time: 0.3873s). Check your callbacks.\n",
      "469/469 [==============================] - 4s 8ms/step - loss: 0.0060 - sparse_categorical_accuracy: 0.9980 - val_loss: 0.0928 - val_sparse_categorical_accuracy: 0.9825\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0035 - sparse_categorical_accuracy: 0.9988 - val_loss: 0.0902 - val_sparse_categorical_accuracy: 0.9840\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0048 - sparse_categorical_accuracy: 0.9983 - val_loss: 0.0960 - val_sparse_categorical_accuracy: 0.9837\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0043 - sparse_categorical_accuracy: 0.9984 - val_loss: 0.0825 - val_sparse_categorical_accuracy: 0.9842\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0056 - sparse_categorical_accuracy: 0.9978 - val_loss: 0.0902 - val_sparse_categorical_accuracy: 0.9826\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0033 - sparse_categorical_accuracy: 0.9988 - val_loss: 0.0903 - val_sparse_categorical_accuracy: 0.9840\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0052 - sparse_categorical_accuracy: 0.9982 - val_loss: 0.0950 - val_sparse_categorical_accuracy: 0.9844\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0040 - sparse_categorical_accuracy: 0.9987 - val_loss: 0.0965 - val_sparse_categorical_accuracy: 0.9829\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0045 - sparse_categorical_accuracy: 0.9984 - val_loss: 0.0882 - val_sparse_categorical_accuracy: 0.9840\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 3s 6ms/step - loss: 0.0040 - sparse_categorical_accuracy: 0.9988 - val_loss: 0.0971 - val_sparse_categorical_accuracy: 0.9830\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0971 - sparse_categorical_accuracy: 0.9830\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.09707936644554138, 0.9829999804496765]"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "# now fitting the model and then will evaluate with the test data\n",
    "all_data_model.fit(x=train_images, y=train_labels, batch_size=128, epochs=10, validation_data=(test_images, test_labels), \n",
    "                    callbacks=[tensorboard_callback])\n",
    "all_data_model.evaluate(x=test_images, y=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4\n784 512 512 10\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((784, 512), (512,), (512, 10), (10,))"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "w = all_data_model.get_weights()\n",
    "print(len(w))\n",
    "print(len(w[0]), len(w[1]), len(w[2]), len(w[3]))\n",
    "w[0].shape, w[1].shape, w[2].shape,   w[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"functional_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         [(None, 784)]             0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 512)               401920    \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 512)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 10)                5130      \n=================================================================\nTotal params: 407,050\nTrainable params: 407,050\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "all_data_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am going to try the three models again, but this time I will \n",
    "# mix in the data so that some of the data is seen by more than one model\n"
   ]
  }
 ]
}